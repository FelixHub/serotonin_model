{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import pyglet\n",
    "from pyglet.window import key\n",
    "\n",
    "import miniworld\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "from miniworld.wrappers import PyTorchObsWrapper,GreyscaleWrapper\n",
    "\n",
    "train_args = dict(\n",
    "    nb_sections=2,\n",
    "    proba_change_motor_gain=0,\n",
    "    min_section_length=5,\n",
    "    max_section_length=10,\n",
    "    training=False,\n",
    "    max_episode_steps=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections limits [-1, 8, 14]\n",
      "sections lengths [9, 6]\n",
      "sections motor gains [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('MiniWorld-Hallway-v0', length = 15, view=\"agent\", render_mode=None)\n",
    "env = gym.make('MiniWorld-TaskHallway-v0', view=\"agent\", render_mode=None,**train_args)\n",
    "# env = GreyscaleWrapper(env)\n",
    "env = PyTorchObsWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.hidden_size = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2).to(device)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2).to(device)\n",
    "        self.maxpool = nn.MaxPool2d(2).to(device)\n",
    "\n",
    "        self.outconvsize = 2016\n",
    "        self.affine1 = nn.Linear( self.outconvsize , self.hidden_size).to(device) \n",
    "\n",
    "        ### replacement for rnn \n",
    "        # self.rnn = nn.RNN(self.outconvsize, self.hidden_size,batch_first=True).to(device)\n",
    "        # self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "        self.action_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 3).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 1).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.batch_loss = []\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    # def reset_hidden_state(self):\n",
    "    #     self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.maxpool(self.conv2(x)))\n",
    "        x = x.reshape(-1,self.outconvsize)\n",
    "        out = self.relu(self.affine1(x))\n",
    "        # h = self.rnn(x.unsqueeze(0), self.hidden_state)[1]\n",
    "        # self.hidden_state = h\n",
    "        # out = h.squeeze(0)\n",
    "        action_prob = F.softmax(self.action_head(out), dim=-1)\n",
    "        state_value = self.value_head(out)\n",
    "        return action_prob, state_value\n",
    "\n",
    "\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs,state_value = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append( (m.log_prob(action), state_value) )\n",
    "\n",
    "    return action.item(),probs\n",
    "    \n",
    "gamma = 1\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + eps) # this create error ...\n",
    "    for (log_prob,value), R in zip(policy.saved_log_probs, returns):\n",
    "        advantage = R - value.item()\n",
    "        policy_loss.append(-log_prob * advantage)\n",
    "        value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n",
    "    loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "    policy.batch_loss.append(loss)\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    # policy.reset_hidden_state()\n",
    "\n",
    "\n",
    "def finish_batch():\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy.batch_loss).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.batch_loss[:]\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71876"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need at least 3000 episodes to get a good training\n",
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 100):  # Don't infinite loop while learning\n",
    "            action,show_prob = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            #if args.render:\n",
    "            #    env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 5 == 0:\n",
    "            loss = finish_batch()\n",
    "            print(loss,show_prob.cpu().detach().numpy()[0])\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11393/767425537.py:82: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.65211296081543 [0.00159358 0.01349463 0.9849118 ]\n",
      "Episode 0\tLast reward: 0.00\tAverage reward: 0.00\n",
      "25.398656845092773 [0.00232973 0.02104751 0.9766227 ]\n",
      "Episode 5\tLast reward: 0.00\tAverage reward: 0.00\n",
      "27.828447341918945 [0.00659609 0.04136344 0.9520405 ]\n",
      "Episode 10\tLast reward: 0.92\tAverage reward: 0.17\n",
      "27.841617584228516 [0.0016092  0.0165321  0.98185873]\n",
      "Episode 15\tLast reward: 0.93\tAverage reward: 0.30\n",
      "33.1246223449707 [0.00309733 0.02533715 0.9715655 ]\n",
      "Episode 20\tLast reward: 0.92\tAverage reward: 0.40\n",
      "56.22462463378906 [0.00359458 0.02791293 0.96849257]\n",
      "Episode 25\tLast reward: 0.00\tAverage reward: 0.39\n",
      "30.714664459228516 [9.3241001e-04 1.1570062e-02 9.8749745e-01]\n",
      "Episode 30\tLast reward: 0.93\tAverage reward: 0.42\n",
      "49.60091018676758 [0.00121854 0.0137844  0.98499703]\n",
      "Episode 35\tLast reward: 0.93\tAverage reward: 0.46\n",
      "13.320852279663086 [0.29054704 0.27555546 0.43389747]\n",
      "Episode 40\tLast reward: 0.00\tAverage reward: 0.48\n",
      "50.74651336669922 [0.00138583 0.01499406 0.98362005]\n",
      "Episode 45\tLast reward: 0.00\tAverage reward: 0.41\n",
      "65.20094299316406 [0.00329705 0.02638829 0.9703147 ]\n",
      "Episode 50\tLast reward: 0.00\tAverage reward: 0.36\n",
      "47.736915588378906 [5.0564867e-04 3.4919814e-03 9.9600238e-01]\n",
      "Episode 55\tLast reward: 0.00\tAverage reward: 0.36\n",
      "-14.899450302124023 [0.00280401 0.01234732 0.9848487 ]\n",
      "Episode 60\tLast reward: 0.00\tAverage reward: 0.28\n",
      "34.12008285522461 [0.00323241 0.02605063 0.97071695]\n",
      "Episode 65\tLast reward: 0.93\tAverage reward: 0.43\n",
      "38.90597915649414 [1.5170680e-04 1.4166295e-03 9.9843162e-01]\n",
      "Episode 70\tLast reward: 0.00\tAverage reward: 0.42\n",
      "46.153236389160156 [0.02888478 0.10572449 0.8653907 ]\n",
      "Episode 75\tLast reward: 0.92\tAverage reward: 0.41\n",
      "41.60404586791992 [0.01117523 0.05805225 0.93077254]\n",
      "Episode 80\tLast reward: 0.93\tAverage reward: 0.48\n",
      "27.70371437072754 [0.00108138 0.00660952 0.9923091 ]\n",
      "Episode 85\tLast reward: 0.00\tAverage reward: 0.46\n",
      "35.241905212402344 [0.01076267 0.05236338 0.9368739 ]\n",
      "Episode 90\tLast reward: 0.92\tAverage reward: 0.52\n",
      "37.55653381347656 [0.00243154 0.01234395 0.9852246 ]\n",
      "Episode 95\tLast reward: 0.94\tAverage reward: 0.61\n",
      "29.58308219909668 [6.5957574e-04 9.2229526e-03 9.9011749e-01]\n",
      "Episode 100\tLast reward: 0.00\tAverage reward: 0.60\n",
      "17.070220947265625 [0.0019651  0.01883583 0.979199  ]\n",
      "Episode 105\tLast reward: 0.93\tAverage reward: 0.59\n",
      "50.39136505126953 [0.0012447  0.01397728 0.98477805]\n",
      "Episode 110\tLast reward: 0.00\tAverage reward: 0.46\n",
      "57.9576416015625 [0.00197921 0.01892397 0.97909683]\n",
      "Episode 115\tLast reward: 0.92\tAverage reward: 0.52\n",
      "16.712615966796875 [0.00156785 0.01625338 0.98217875]\n",
      "Episode 120\tLast reward: 0.00\tAverage reward: 0.40\n",
      "23.94245147705078 [0.00116515 0.01338639 0.98544854]\n",
      "Episode 125\tLast reward: 0.92\tAverage reward: 0.44\n",
      "48.19814682006836 [9.0571493e-04 6.8162465e-03 9.9227798e-01]\n",
      "Episode 130\tLast reward: 0.00\tAverage reward: 0.46\n",
      "33.377403259277344 [0.00123935 0.01393797 0.98482263]\n",
      "Episode 135\tLast reward: 0.00\tAverage reward: 0.35\n",
      "4.852878570556641 [1.9556901e-04 1.5498649e-03 9.9825460e-01]\n",
      "Episode 140\tLast reward: 0.00\tAverage reward: 0.31\n",
      "22.04842758178711 [0.00244845 0.01139183 0.98615974]\n",
      "Episode 145\tLast reward: 0.00\tAverage reward: 0.32\n",
      "25.514179229736328 [0.00211175 0.0197415  0.97814673]\n",
      "Episode 150\tLast reward: 0.92\tAverage reward: 0.38\n",
      "59.01162338256836 [0.02938035 0.02381502 0.9468047 ]\n",
      "Episode 155\tLast reward: 0.92\tAverage reward: 0.42\n",
      "-12.753277778625488 [0.00413109 0.03055234 0.9653166 ]\n",
      "Episode 160\tLast reward: 0.93\tAverage reward: 0.45\n",
      "-2.113676071166992 [0.00575517 0.03787434 0.95637053]\n",
      "Episode 165\tLast reward: 0.00\tAverage reward: 0.39\n",
      "0.4862346649169922 [0.34339476 0.308127   0.34847826]\n",
      "Episode 170\tLast reward: 0.00\tAverage reward: 0.30\n",
      "49.01163101196289 [8.8854518e-04 1.1210760e-02 9.8790073e-01]\n",
      "Episode 175\tLast reward: 0.00\tAverage reward: 0.32\n",
      "15.504308700561523 [0.00164604 0.0089105  0.98944354]\n",
      "Episode 180\tLast reward: 0.00\tAverage reward: 0.33\n",
      "22.086292266845703 [0.00153061 0.01600005 0.9824693 ]\n",
      "Episode 185\tLast reward: 0.93\tAverage reward: 0.34\n",
      "10.788570404052734 [5.917056e-04 8.589444e-03 9.908188e-01]\n",
      "Episode 190\tLast reward: 0.00\tAverage reward: 0.35\n",
      "0.44334888458251953 [0.032478   0.02640229 0.9411198 ]\n",
      "Episode 195\tLast reward: 0.00\tAverage reward: 0.31\n",
      "22.60930633544922 [0.00319599 0.02585936 0.9709447 ]\n",
      "Episode 200\tLast reward: 0.93\tAverage reward: 0.37\n",
      "45.75450134277344 [0.00167453 0.01696746 0.981358  ]\n",
      "Episode 205\tLast reward: 0.93\tAverage reward: 0.45\n",
      "37.223670959472656 [0.00366542 0.02826918 0.9680654 ]\n",
      "Episode 210\tLast reward: 0.93\tAverage reward: 0.48\n",
      "12.657084465026855 [0.00195077 0.01874611 0.9793032 ]\n",
      "Episode 215\tLast reward: 0.00\tAverage reward: 0.46\n",
      "19.94733428955078 [0.00213933 0.0172462  0.9806144 ]\n",
      "Episode 220\tLast reward: 0.00\tAverage reward: 0.44\n",
      "51.15352249145508 [8.2862063e-04 1.0709754e-02 9.8846161e-01]\n",
      "Episode 225\tLast reward: 0.93\tAverage reward: 0.39\n",
      "40.658607482910156 [0.01181865 0.06016777 0.92801356]\n",
      "Episode 230\tLast reward: 0.93\tAverage reward: 0.47\n",
      "48.733314514160156 [3.4988840e-04 2.6227045e-03 9.9702746e-01]\n",
      "Episode 235\tLast reward: 0.00\tAverage reward: 0.45\n",
      "41.01925277709961 [0.00130494 0.01441597 0.9842792 ]\n",
      "Episode 240\tLast reward: 0.00\tAverage reward: 0.43\n",
      "61.45588302612305 [0.00888175 0.0501014  0.94101685]\n",
      "Episode 245\tLast reward: 0.00\tAverage reward: 0.46\n",
      "39.89662551879883 [0.00237859 0.02133424 0.9762872 ]\n",
      "Episode 250\tLast reward: 0.00\tAverage reward: 0.44\n",
      "43.72184753417969 [9.5893943e-04 1.1784494e-02 9.8725653e-01]\n",
      "Episode 255\tLast reward: 0.93\tAverage reward: 0.43\n",
      "0.6329269409179688 [0.00759397 0.04529763 0.9471085 ]\n",
      "Episode 260\tLast reward: 0.93\tAverage reward: 0.46\n",
      "44.88328552246094 [0.03055499 0.10946998 0.8599751 ]\n",
      "Episode 265\tLast reward: 0.93\tAverage reward: 0.53\n",
      "47.264244079589844 [0.00764028 0.04547539 0.9468844 ]\n",
      "Episode 270\tLast reward: 0.00\tAverage reward: 0.49\n",
      "46.44719696044922 [0.00231122 0.02093835 0.97675055]\n",
      "Episode 275\tLast reward: 0.00\tAverage reward: 0.47\n",
      "54.5010986328125 [0.00258224 0.0225075  0.9749102 ]\n",
      "Episode 280\tLast reward: 0.93\tAverage reward: 0.49\n",
      "55.33173370361328 [0.00535157 0.01776651 0.9768819 ]\n",
      "Episode 285\tLast reward: 0.93\tAverage reward: 0.50\n",
      "2.2397351264953613 [0.00198983 0.01899022 0.97901994]\n",
      "Episode 290\tLast reward: 0.93\tAverage reward: 0.48\n",
      "40.82418441772461 [5.6019827e-04 8.2868356e-03 9.9115294e-01]\n",
      "Episode 295\tLast reward: 0.00\tAverage reward: 0.41\n",
      "29.306358337402344 [0.00190952 0.01848642 0.97960407]\n",
      "Episode 300\tLast reward: 0.00\tAverage reward: 0.44\n",
      "49.3526496887207 [0.00175908 0.01752217 0.98071885]\n",
      "Episode 305\tLast reward: 0.93\tAverage reward: 0.47\n",
      "26.836013793945312 [0.00135623 0.00472863 0.993915  ]\n",
      "Episode 310\tLast reward: 0.00\tAverage reward: 0.45\n",
      "11.775842666625977 [0.00502822 0.01237207 0.98259974]\n",
      "Episode 315\tLast reward: 0.00\tAverage reward: 0.39\n",
      "32.48583221435547 [0.00149135 0.00399601 0.9945127 ]\n",
      "Episode 320\tLast reward: 0.00\tAverage reward: 0.34\n",
      "68.93470001220703 [0.0014078  0.01514902 0.9834432 ]\n",
      "Episode 325\tLast reward: 0.93\tAverage reward: 0.35\n",
      "-18.474079132080078 [0.00164805 0.01679173 0.98156023]\n",
      "Episode 330\tLast reward: 0.00\tAverage reward: 0.31\n",
      "92.22523498535156 [0.00233443 0.0210752  0.9765904 ]\n",
      "Episode 335\tLast reward: 0.00\tAverage reward: 0.24\n",
      "32.36077117919922 [0.0071239  0.02997476 0.96290135]\n",
      "Episode 340\tLast reward: 0.93\tAverage reward: 0.36\n",
      "10.72330093383789 [0.01377243 0.06633566 0.9198919 ]\n",
      "Episode 345\tLast reward: 0.00\tAverage reward: 0.35\n",
      "22.114601135253906 [0.0020498  0.01936171 0.9785886 ]\n",
      "Episode 350\tLast reward: 0.92\tAverage reward: 0.45\n",
      "35.65057373046875 [0.62909704 0.00955781 0.36134514]\n",
      "Episode 355\tLast reward: 0.00\tAverage reward: 0.35\n",
      "17.549110412597656 [0.04398797 0.02118636 0.9348256 ]\n",
      "Episode 360\tLast reward: 0.00\tAverage reward: 0.27\n",
      "49.07630157470703 [7.122288e-04 9.698920e-03 9.895890e-01]\n",
      "Episode 365\tLast reward: 0.00\tAverage reward: 0.29\n",
      "70.26423645019531 [6.6116400e-04 9.2374990e-03 9.9010134e-01]\n",
      "Episode 370\tLast reward: 0.00\tAverage reward: 0.26\n",
      "54.95313262939453 [0.00325242 0.02615544 0.9705921 ]\n",
      "Episode 375\tLast reward: 0.93\tAverage reward: 0.34\n",
      "40.268150329589844 [0.00158145 0.01634532 0.9820733 ]\n",
      "Episode 380\tLast reward: 0.92\tAverage reward: 0.39\n",
      "42.71810531616211 [7.8187400e-04 1.0310118e-02 9.8890799e-01]\n",
      "Episode 385\tLast reward: 0.93\tAverage reward: 0.43\n",
      "47.88142395019531 [5.4692681e-04 3.2505121e-03 9.9620265e-01]\n",
      "Episode 390\tLast reward: 0.00\tAverage reward: 0.42\n",
      "53.215003967285156 [0.023655   0.09335312 0.88299197]\n",
      "Episode 395\tLast reward: 0.92\tAverage reward: 0.45\n",
      "23.367216110229492 [0.90972316 0.00591267 0.08436418]\n",
      "Episode 400\tLast reward: 0.00\tAverage reward: 0.39\n",
      "22.388463973999023 [0.00925582 0.02787072 0.9628735 ]\n",
      "Episode 405\tLast reward: 0.00\tAverage reward: 0.38\n",
      "19.356626510620117 [0.0018617  0.01818293 0.9799554 ]\n",
      "Episode 410\tLast reward: 0.92\tAverage reward: 0.42\n",
      "34.8719596862793 [0.0018396  0.00909932 0.9890611 ]\n",
      "Episode 415\tLast reward: 0.00\tAverage reward: 0.41\n",
      "43.984825134277344 [0.00191865 0.01854409 0.97953725]\n",
      "Episode 420\tLast reward: 0.92\tAverage reward: 0.45\n",
      "81.74564361572266 [0.00329473 0.02637622 0.97032905]\n",
      "Episode 425\tLast reward: 0.00\tAverage reward: 0.43\n",
      "29.592260360717773 [0.00604933 0.03292354 0.96102715]\n",
      "Episode 430\tLast reward: 0.93\tAverage reward: 0.46\n",
      "43.5672607421875 [0.00253885 0.02226037 0.9752008 ]\n",
      "Episode 435\tLast reward: 0.94\tAverage reward: 0.49\n",
      "40.95219039916992 [0.01770937 0.07760677 0.9046839 ]\n",
      "Episode 440\tLast reward: 0.92\tAverage reward: 0.46\n",
      "55.03484344482422 [0.01258248 0.03309652 0.95432097]\n",
      "Episode 445\tLast reward: 0.92\tAverage reward: 0.40\n",
      "33.0418701171875 [0.00553151 0.03691553 0.9575529 ]\n",
      "Episode 450\tLast reward: 0.00\tAverage reward: 0.43\n",
      "43.235103607177734 [0.01495954 0.06991748 0.9151229 ]\n",
      "Episode 455\tLast reward: 0.92\tAverage reward: 0.50\n",
      "39.0492057800293 [0.00099881 0.01210296 0.9868982 ]\n",
      "Episode 460\tLast reward: 0.93\tAverage reward: 0.52\n",
      "42.251930236816406 [0.00249677 0.01621742 0.9812858 ]\n",
      "Episode 465\tLast reward: 0.00\tAverage reward: 0.48\n",
      "10.069658279418945 [0.00105518 0.01254566 0.98639905]\n",
      "Episode 470\tLast reward: 0.92\tAverage reward: 0.46\n",
      "41.89105987548828 [0.00388162 0.0293414  0.966777  ]\n",
      "Episode 475\tLast reward: 0.93\tAverage reward: 0.45\n",
      "45.38176727294922 [0.00278835 0.02228197 0.9749297 ]\n",
      "Episode 480\tLast reward: 0.00\tAverage reward: 0.43\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), '../saved_models/miniworld_agent_09042023.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "policy.load_state_dict(torch.load('../saved_models/miniworld_task.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections limits [-1, 4, 12, 17]\n",
      "sections lengths [5, 8, 5]\n",
      "sections motor gains [1, 1, 1]\n",
      "tensor([[0.0252, 0.0631, 0.9117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0295, 0.0632, 0.9073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0198, 0.0489, 0.9313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0160, 0.0431, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0076, 0.0210, 0.9714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0055, 0.0119, 0.9826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0117, 0.0255, 0.9627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0075, 0.0226, 0.9699]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0063, 0.0182, 0.9755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0043, 0.0096, 0.9862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0079, 0.0200, 0.9721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0049, 0.0102, 0.9849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0109, 0.0334, 0.9557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0048, 0.0134, 0.9818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0085, 0.0239, 0.9676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0047, 0.0117, 0.9836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0044, 0.0130, 0.9826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0019, 0.0075, 0.9905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0043, 0.0133, 0.9823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0041, 0.0087, 0.9872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0071, 0.0164, 0.9765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0082, 0.0223, 0.9695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0099, 0.0304, 0.9597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0065, 0.0140, 0.9795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0080, 0.0194, 0.9726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0107, 0.0263, 0.9630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0052, 0.0152, 0.9796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0079, 0.0218, 0.9704]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0057, 0.0131, 0.9812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0237, 0.0628, 0.9134]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0105, 0.0245, 0.9651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0073, 0.0220, 0.9707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0061, 0.0122, 0.9817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0118, 0.0207, 0.9674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0126, 0.0285, 0.9589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0042, 0.0134, 0.9824]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0133, 0.0320, 0.9547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0074, 0.0123, 0.9803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0082, 0.0153, 0.9765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0060, 0.0109, 0.9832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0055, 0.0181, 0.9764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0089, 0.0190, 0.9721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0354, 0.0099, 0.9548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0140, 0.0225, 0.9635]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0111, 0.0198, 0.9691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0237, 0.0445, 0.9318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0130, 0.0272, 0.9598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3264, 0.0145, 0.6591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0290, 0.0262, 0.9448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0409, 0.0254, 0.9336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1130, 0.0351, 0.8519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0186, 0.0395, 0.9419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.9319e-01, 6.2391e-04, 6.1907e-03]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[4.7022e-04, 2.9137e-03, 9.9662e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.3813e-04, 3.6176e-03, 9.9574e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.4720e-04, 4.1634e-03, 9.9519e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.9597e-04, 2.2423e-03, 9.9736e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.9992e-04, 1.6445e-03, 9.9816e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.9602e-04, 2.3298e-03, 9.9727e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.8799e-04, 2.6562e-03, 9.9696e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.7985e-04, 2.4735e-03, 9.9715e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.3149e-04, 1.1135e-03, 9.9876e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.0435e-04, 1.6583e-03, 9.9814e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.6081e-04, 2.5791e-03, 9.9706e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[4.0707e-04, 3.0129e-03, 9.9658e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.0609e-04, 1.5797e-03, 9.9821e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.9078e-04, 2.2539e-03, 9.9746e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.8897e-04, 4.1633e-03, 9.9525e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.2021e-04, 5.1940e-03, 9.9419e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.1017e-04, 5.5719e-03, 9.9362e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0013, 0.0118, 0.9868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.2135e-04, 8.8352e-03, 9.9034e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.0112e-04, 8.6788e-03, 9.9072e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[7.9662e-04, 1.0437e-02, 9.8877e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.2618e-04, 1.0689e-02, 9.8848e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.7791e-04, 1.1123e-02, 9.8800e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.2970e-04, 8.9471e-03, 9.9042e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0010, 0.0121, 0.9870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0018, 0.0180, 0.9802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0023, 0.0210, 0.9766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0020, 0.0193, 0.9787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.0127, 0.9862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0013, 0.0145, 0.9842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.5005e-04, 9.1354e-03, 9.9021e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.9181e-04, 6.5550e-03, 9.9305e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.4217e-04, 5.9977e-03, 9.9366e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.0848e-04, 7.7769e-03, 9.9171e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.5576e-04, 6.1528e-03, 9.9349e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.3855e-04, 1.1620e-02, 9.8744e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0097, 0.0529, 0.9374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "over\n",
      "tensor([[0.0256, 0.0616, 0.9127]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0372, 0.1034, 0.8593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0356, 0.0903, 0.8741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0205, 0.0616, 0.9179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0442, 0.1235, 0.8323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0411, 0.0966, 0.8623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0095, 0.0315, 0.9590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0073, 0.0242, 0.9684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0209, 0.0607, 0.9185]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0209, 0.0616, 0.9175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0183, 0.0444, 0.9372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0106, 0.0298, 0.9596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0157, 0.0413, 0.9430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0222, 0.0562, 0.9216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0766, 0.1897, 0.7337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0222, 0.0562, 0.9216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0155, 0.0435, 0.9410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0347, 0.0797, 0.8857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0360, 0.1053, 0.8587]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0309, 0.0901, 0.8790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0663, 0.1336, 0.8001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0159, 0.0407, 0.9434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0256, 0.9624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0074, 0.0175, 0.9751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0226, 0.0626, 0.9148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0105, 0.0231, 0.9664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0078, 0.0184, 0.9739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0276, 0.0738, 0.8986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0066, 0.0203, 0.9731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0109, 0.0205, 0.9687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0121, 0.0333, 0.9547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0109, 0.0285, 0.9606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0087, 0.0177, 0.9737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0056, 0.0137, 0.9807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0089, 0.0241, 0.9670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0062, 0.0178, 0.9760]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0056, 0.0134, 0.9810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0042, 0.0114, 0.9844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0076, 0.0234, 0.9690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0038, 0.0124, 0.9838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0063, 0.0145, 0.9791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0078, 0.0218, 0.9704]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0039, 0.0143, 0.9818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0064, 0.0153, 0.9782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0072, 0.0227, 0.9702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0051, 0.0154, 0.9795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0038, 0.0141, 0.9820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0065, 0.0200, 0.9735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0024, 0.0091, 0.9884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0024, 0.0068, 0.9908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0032, 0.0134, 0.9834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0017, 0.0073, 0.9909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0016, 0.0055, 0.9929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0014, 0.0054, 0.9932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.0064, 0.9925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.5286e-04, 3.3092e-03, 9.9614e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0010, 0.0050, 0.9940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0012, 0.0062, 0.9926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.2029e-04, 6.1326e-03, 9.9295e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[7.2121e-04, 4.7152e-03, 9.9456e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0013, 0.0074, 0.9913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0013, 0.0084, 0.9903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[4.0940e-04, 3.3974e-03, 9.9619e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0015, 0.0098, 0.9887]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0022, 0.0122, 0.9856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0014, 0.0086, 0.9900]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0021, 0.0199, 0.9780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0024, 0.0215, 0.9761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0016, 0.0163, 0.9821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0028, 0.0236, 0.9737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.0000e+00, 1.2797e-09, 1.4133e-10]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0028, 0.0236, 0.9737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0014, 0.0148, 0.9838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0017, 0.0172, 0.9811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0036, 0.0280, 0.9684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0036, 0.0271, 0.9692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0052, 0.0206, 0.9742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0971, 0.0279, 0.8750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0613, 0.0408, 0.8979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0493, 0.0250, 0.9257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8413, 0.0031, 0.1556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0016, 0.0165, 0.9819]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.8649e-04, 8.5398e-03, 9.9087e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.0166e-04, 4.2390e-03, 9.9556e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.3742e-04, 3.2952e-03, 9.9657e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.1537e-04, 2.9376e-03, 9.9695e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.4853e-04, 6.0706e-03, 9.9358e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.9472e-04, 9.5420e-03, 9.8976e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.1880e-04, 1.1459e-02, 9.8762e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0028, 0.0236, 0.9736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "over\n",
      "tensor([[0.0310, 0.0643, 0.9048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0814, 0.1372, 0.7814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0762, 0.1470, 0.7767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0895, 0.1364, 0.7740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0926, 0.1454, 0.7620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0883, 0.1383, 0.7734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0421, 0.0911, 0.8667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0311, 0.0682, 0.9007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0338, 0.0717, 0.8946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0546, 0.0918, 0.8535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0412, 0.0938, 0.8651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0369, 0.0937, 0.8694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0507, 0.1251, 0.8243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0622, 0.1410, 0.7968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0552, 0.1200, 0.8248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0465, 0.0961, 0.8573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0242, 0.0569, 0.9188]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0335, 0.0649, 0.9016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0584, 0.1204, 0.8211]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0810, 0.1683, 0.7507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0195, 0.0544, 0.9261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0275, 0.0641, 0.9084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0178, 0.0391, 0.9431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0109, 0.0293, 0.9597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0276, 0.0637, 0.9087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0359, 0.0722, 0.8919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0312, 0.9569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_args = dict(\n",
    "    nb_sections=3,\n",
    "    proba_change_motor_gain=0,\n",
    "    min_section_length=5,\n",
    "    max_section_length=10,\n",
    "    training=False,\n",
    "    max_episode_steps=250,\n",
    ")\n",
    "env = gym.make('MiniWorld-TaskHallway-v0', view=\"agent\", render_mode=\"human\",**test_args)\n",
    "# env = gym.make('MiniWorld-Hallway-v0', length = 15, view=\"agent\", render_mode=\"human\")\n",
    "env = PyTorchObsWrapper(env)\n",
    "\n",
    "import timeit\n",
    "class CodeTimer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (timeit.default_timer() - self.start) * 1000.0\n",
    "        print('Code block' + self.name + ' took: ' + str(self.took) + ' ms')\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Create the display window\n",
    "env.render()\n",
    "\n",
    "for _ in range(test_args['max_episode_steps']):\n",
    "    action,probs = select_action(observation) # agent policy that uses the observation and info\n",
    "    print(probs)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        print('over')\n",
    "        observation, info = env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m policy\u001b[39m.\u001b[39mvalue_head[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(policy\u001b[39m.\u001b[39mvalue_head[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "policy.value_head[0].weight\n",
    "plt.imshow(policy.value_head[0].weight.cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy.action_head[0].weight\n",
    "plt.imshow(policy.action_head[0].weight.cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c269a8ac1ce32886f1d361a7d7752f19c27b7d8ebd8d0fb0e1cb07b13dddb563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
