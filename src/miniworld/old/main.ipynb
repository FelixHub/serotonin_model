{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import pyglet\n",
    "from pyglet.window import key\n",
    "\n",
    "import miniworld\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "from miniworld.wrappers import PyTorchObsWrapper,GreyscaleWrapper\n",
    "\n",
    "train_args = dict(\n",
    "    nb_sections=2,\n",
    "    proba_change_motor_gain=0,\n",
    "    min_section_length=5,\n",
    "    max_section_length=10,\n",
    "    training=True,\n",
    "    max_episode_steps=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections limits [-1, 4, 9]\n",
      "sections lengths [5, 5]\n",
      "sections motor gains [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('MiniWorld-Hallway-v0', length = 15, view=\"agent\", render_mode=None)\n",
    "env = gym.make('MiniWorld-TaskHallway-v0', view=\"agent\", render_mode=None,**train_args)\n",
    "# env = GreyscaleWrapper(env)\n",
    "env = PyTorchObsWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.hidden_size = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2).to(device)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2).to(device)\n",
    "        self.maxpool = nn.MaxPool2d(2).to(device)\n",
    "\n",
    "        self.outconvsize = 2016\n",
    "        self.affine1 = nn.Linear( self.outconvsize , self.hidden_size).to(device) \n",
    "\n",
    "        ### replacement for rnn \n",
    "        # self.rnn = nn.RNN(self.outconvsize, self.hidden_size,batch_first=True).to(device)\n",
    "        # self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "        self.action_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 3).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 1).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.batch_loss = []\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    # def reset_hidden_state(self):\n",
    "    #     self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.maxpool(self.conv2(x)))\n",
    "        x = x.reshape(-1,self.outconvsize)\n",
    "        out = self.relu(self.affine1(x))\n",
    "        # h = self.rnn(x.unsqueeze(0), self.hidden_state)[1]\n",
    "        # self.hidden_state = h\n",
    "        # out = h.squeeze(0)\n",
    "        action_prob = F.softmax(self.action_head(out), dim=-1)\n",
    "        state_value = self.value_head(out)\n",
    "        return action_prob, state_value\n",
    "\n",
    "\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs,state_value = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append( (m.log_prob(action), state_value) )\n",
    "\n",
    "    return action.item(),probs\n",
    "    \n",
    "gamma = 1\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + eps) # this create error ...\n",
    "    for (log_prob,value), R in zip(policy.saved_log_probs, returns):\n",
    "        advantage = R - value.item()\n",
    "        policy_loss.append(-log_prob * advantage)\n",
    "        value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n",
    "    loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "    policy.batch_loss.append(loss)\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    # policy.reset_hidden_state()\n",
    "\n",
    "\n",
    "def finish_batch():\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy.batch_loss).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.batch_loss[:]\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71876"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need at least 3000 episodes to get a good training\n",
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 100):  # Don't infinite loop while learning\n",
    "            action,show_prob = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            #if args.render:\n",
    "            #    env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 5 == 0:\n",
    "            loss = finish_batch()\n",
    "            print(loss,show_prob.cpu().detach().numpy()[0])\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fphub\\AppData\\Local\\Temp\\ipykernel_23684\\767425537.py:82: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.47510528564453 [0.03931928 0.14655508 0.81412566]\n",
      "Episode 0\tLast reward: 0.95\tAverage reward: 0.05\n",
      "76.29138946533203 [0.04505858 0.06650766 0.8884338 ]\n",
      "Episode 5\tLast reward: 0.95\tAverage reward: 0.17\n",
      "23.9126033782959 [0.03524425 0.05895958 0.9057962 ]\n",
      "Episode 10\tLast reward: 0.00\tAverage reward: 0.26\n",
      "-14.236412048339844 [0.04074701 0.12288248 0.8363705 ]\n",
      "Episode 15\tLast reward: 0.00\tAverage reward: 0.20\n",
      "47.92357635498047 [0.02786716 0.03374393 0.9383889 ]\n",
      "Episode 20\tLast reward: 0.97\tAverage reward: 0.30\n",
      "12.521570205688477 [0.0704511  0.35273457 0.5768143 ]\n",
      "Episode 25\tLast reward: 0.00\tAverage reward: 0.27\n",
      "27.331180572509766 [0.03733324 0.12429001 0.8383767 ]\n",
      "Episode 30\tLast reward: 0.96\tAverage reward: 0.30\n",
      "17.748172760009766 [0.02617682 0.06783593 0.9059873 ]\n",
      "Episode 35\tLast reward: 0.98\tAverage reward: 0.42\n",
      "80.11885070800781 [0.03760736 0.07959666 0.882796  ]\n",
      "Episode 40\tLast reward: 0.95\tAverage reward: 0.54\n",
      "76.74993133544922 [0.03490482 0.03338529 0.9317099 ]\n",
      "Episode 45\tLast reward: 0.97\tAverage reward: 0.51\n",
      "124.45703887939453 [0.035972   0.07698619 0.8870418 ]\n",
      "Episode 50\tLast reward: 0.99\tAverage reward: 0.53\n",
      "-1.0804564952850342 [0.0711486  0.10232277 0.8265286 ]\n",
      "Episode 55\tLast reward: 1.00\tAverage reward: 0.55\n",
      "1.8323841094970703 [0.0180595  0.03540898 0.9465315 ]\n",
      "Episode 60\tLast reward: 0.99\tAverage reward: 0.51\n",
      "-7.7495503425598145 [0.04155805 0.13406084 0.82438105]\n",
      "Episode 65\tLast reward: 0.00\tAverage reward: 0.49\n",
      "18.03204917907715 [0.02936337 0.0965173  0.8741193 ]\n",
      "Episode 70\tLast reward: 0.00\tAverage reward: 0.42\n",
      "65.54740905761719 [0.04770863 0.1799635  0.7723279 ]\n",
      "Episode 75\tLast reward: 0.00\tAverage reward: 0.50\n",
      "9.87476921081543 [0.04113946 0.09444594 0.86441463]\n",
      "Episode 80\tLast reward: 1.00\tAverage reward: 0.52\n",
      "-6.186969757080078 [0.10875564 0.27993962 0.6113047 ]\n",
      "Episode 85\tLast reward: 0.00\tAverage reward: 0.54\n",
      "100.2138671875 [0.03066401 0.11995257 0.8493835 ]\n",
      "Episode 90\tLast reward: 0.00\tAverage reward: 0.59\n",
      "37.723201751708984 [0.03840989 0.09330019 0.8682898 ]\n",
      "Episode 95\tLast reward: 0.97\tAverage reward: 0.59\n",
      "43.11326599121094 [0.04884408 0.1500431  0.8011128 ]\n",
      "Episode 100\tLast reward: 1.00\tAverage reward: 0.59\n",
      "97.50236511230469 [0.03423048 0.06150412 0.9042654 ]\n",
      "Episode 105\tLast reward: 0.94\tAverage reward: 0.64\n",
      "51.35527038574219 [0.06111132 0.14577755 0.7931112 ]\n",
      "Episode 110\tLast reward: 1.00\tAverage reward: 0.67\n",
      "49.339447021484375 [0.06220794 0.15015511 0.787637  ]\n",
      "Episode 115\tLast reward: 0.00\tAverage reward: 0.69\n",
      "55.61205291748047 [0.01736779 0.06090479 0.9217274 ]\n",
      "Episode 120\tLast reward: 0.00\tAverage reward: 0.67\n",
      "-0.5371637344360352 [0.02849209 0.06197213 0.9095358 ]\n",
      "Episode 125\tLast reward: 0.00\tAverage reward: 0.60\n",
      "57.983055114746094 [0.03267528 0.09518752 0.8721372 ]\n",
      "Episode 130\tLast reward: 0.98\tAverage reward: 0.64\n",
      "42.91539001464844 [0.05158384 0.15850954 0.7899067 ]\n",
      "Episode 135\tLast reward: 0.97\tAverage reward: 0.63\n",
      "18.89870834350586 [0.02774079 0.07576797 0.89649117]\n",
      "Episode 140\tLast reward: 0.00\tAverage reward: 0.62\n",
      "1.918916940689087 [0.05703472 0.15566635 0.7872989 ]\n",
      "Episode 145\tLast reward: 0.99\tAverage reward: 0.57\n",
      "102.02986907958984 [0.07185537 0.16238403 0.76576066]\n",
      "Episode 150\tLast reward: 0.00\tAverage reward: 0.52\n",
      "13.306288719177246 [0.06542756 0.08711769 0.8474547 ]\n",
      "Episode 155\tLast reward: 1.00\tAverage reward: 0.58\n",
      "16.999378204345703 [0.03802412 0.15306684 0.808909  ]\n",
      "Episode 160\tLast reward: 0.00\tAverage reward: 0.58\n",
      "47.52515411376953 [0.05050906 0.19758382 0.7519071 ]\n",
      "Episode 165\tLast reward: 0.00\tAverage reward: 0.53\n",
      "50.997955322265625 [0.04401902 0.20486519 0.75111574]\n",
      "Episode 170\tLast reward: 0.00\tAverage reward: 0.50\n",
      "27.295299530029297 [0.02846338 0.06963275 0.90190387]\n",
      "Episode 175\tLast reward: 0.00\tAverage reward: 0.51\n",
      "82.73748779296875 [0.01451762 0.06586838 0.919614  ]\n",
      "Episode 180\tLast reward: 0.00\tAverage reward: 0.52\n",
      "55.71851348876953 [0.04462619 0.1284698  0.82690394]\n",
      "Episode 185\tLast reward: 0.98\tAverage reward: 0.58\n",
      "37.58267593383789 [0.05946634 0.14528275 0.79525083]\n",
      "Episode 190\tLast reward: 0.00\tAverage reward: 0.54\n",
      "58.913761138916016 [0.05513237 0.13043968 0.814428  ]\n",
      "Episode 195\tLast reward: 0.99\tAverage reward: 0.59\n",
      "10.685826301574707 [0.08831479 0.21554965 0.69613546]\n",
      "Episode 200\tLast reward: 0.00\tAverage reward: 0.59\n",
      "69.09613037109375 [0.03736221 0.08803764 0.8746002 ]\n",
      "Episode 205\tLast reward: 0.97\tAverage reward: 0.63\n",
      "16.182233810424805 [0.01289913 0.05893455 0.9281664 ]\n",
      "Episode 210\tLast reward: 0.00\tAverage reward: 0.62\n",
      "22.420610427856445 [0.05096078 0.15505406 0.79398507]\n",
      "Episode 215\tLast reward: 0.00\tAverage reward: 0.61\n",
      "78.1292495727539 [0.02568447 0.05362802 0.92068756]\n",
      "Episode 220\tLast reward: 1.00\tAverage reward: 0.64\n",
      "57.05854415893555 [0.0344683  0.03587747 0.9296542 ]\n",
      "Episode 225\tLast reward: 0.98\tAverage reward: 0.63\n",
      "60.794864654541016 [0.02723766 0.04342476 0.92933756]\n",
      "Episode 230\tLast reward: 0.95\tAverage reward: 0.58\n",
      "15.702839851379395 [0.03233533 0.12895162 0.838713  ]\n",
      "Episode 235\tLast reward: 0.00\tAverage reward: 0.54\n",
      "100.51449584960938 [0.06518119 0.09661932 0.83819956]\n",
      "Episode 240\tLast reward: 0.95\tAverage reward: 0.55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m):  \u001b[39m# Don't infinite loop while learning\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     action,show_prob \u001b[39m=\u001b[39m select_action(state)\n\u001b[1;32m----> 9\u001b[0m     state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     10\u001b[0m     \u001b[39m#if args.render:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m#    env.render()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     policy\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\gymnasium\\core.py:422\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    419\u001b[0m     \u001b[39mself\u001b[39m, action: ActType\n\u001b[0;32m    420\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    423\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:51\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     41\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:38\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\users\\fphub\\documents\\ibl\\miniworld\\miniworld\\envs\\taskHallway.py:115\u001b[0m, in \u001b[0;36mTaskHallway.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m--> 115\u001b[0m     obs, reward, termination, truncation, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox):\n\u001b[0;32m    118\u001b[0m         reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reward()\n",
      "File \u001b[1;32mc:\\users\\fphub\\documents\\ibl\\miniworld\\miniworld\\miniworld.py:726\u001b[0m, in \u001b[0;36mMiniWorldEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mcarrying\u001b[39m.\u001b[39mdir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mdir\n\u001b[0;32m    725\u001b[0m \u001b[39m# Generate the current camera image\u001b[39;00m\n\u001b[1;32m--> 726\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_obs()\n\u001b[0;32m    728\u001b[0m \u001b[39m# If the maximum time step count is reached\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_count \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_episode_steps:\n",
      "File \u001b[1;32mc:\\users\\fphub\\documents\\ibl\\miniworld\\miniworld\\miniworld.py:1183\u001b[0m, in \u001b[0;36mMiniWorldEnv.render_obs\u001b[1;34m(self, frame_buffer)\u001b[0m\n\u001b[0;32m   1181\u001b[0m glClearColor(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msky_color, \u001b[39m1.0\u001b[39m)\n\u001b[0;32m   1182\u001b[0m glClearDepth(\u001b[39m1.0\u001b[39m)\n\u001b[1;32m-> 1183\u001b[0m glClear(GL_COLOR_BUFFER_BIT \u001b[39m|\u001b[39;49m GL_DEPTH_BUFFER_BIT)\n\u001b[0;32m   1185\u001b[0m \u001b[39m# Set the projection matrix\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m glMatrixMode(GL_PROJECTION)\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\pyglet\\gl\\lib.py:87\u001b[0m, in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mGLException\u001b[39;00m(\u001b[39mException\u001b[39;00m):\n\u001b[0;32m     84\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrcheck\u001b[39m(result, func, arguments):\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m _debug_gl_trace:\n\u001b[0;32m     89\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), '../saved_models/miniworld_agent_09042023.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "policy.load_state_dict(torch.load('../saved_models/miniworld_agent_09042023.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections limits [-1, 6, 11, 20]\n",
      "sections lengths [7, 5, 9]\n",
      "sections motor gains [1, 1, 1]\n",
      "tensor([[0.1310, 0.2161, 0.6528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2200, 0.2478, 0.5321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2360, 0.2537, 0.5103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1132, 0.1756, 0.7112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3268, 0.3173, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1132, 0.1756, 0.7112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1845, 0.2617, 0.5539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2162, 0.2446, 0.5392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1005, 0.1476, 0.7519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2648, 0.2825, 0.4527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5036, 0.2064, 0.2901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2054, 0.2526, 0.5420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5036, 0.2064, 0.2901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2054, 0.2526, 0.5420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2102, 0.1900, 0.5998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2862, 0.3126, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2022, 0.2558, 0.5420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2862, 0.3126, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2022, 0.2558, 0.5420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2862, 0.3126, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2102, 0.1900, 0.5998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2862, 0.3126, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1666, 0.2141, 0.6193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2017, 0.2181, 0.5802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2017, 0.2181, 0.5802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1097, 0.1558, 0.7345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1773, 0.1769, 0.6458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0708, 0.0952, 0.8340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2883, 0.2887, 0.4230]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2127, 0.2579, 0.5294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1917, 0.2458, 0.5624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2390, 0.2530, 0.5079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1917, 0.2458, 0.5624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2390, 0.2530, 0.5079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1836, 0.1856, 0.6308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3236, 0.2986, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1851, 0.1999, 0.6150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2035, 0.2210, 0.5755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2454, 0.2841, 0.4705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3448, 0.3204, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3448, 0.3204, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3267, 0.3297, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1747, 0.2269, 0.5984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1806, 0.2115, 0.6080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2319, 0.2593, 0.5088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2319, 0.2593, 0.5088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2999, 0.2919, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2999, 0.2919, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2999, 0.2919, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2999, 0.2919, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2229, 0.2648, 0.5123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3086, 0.3244, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3086, 0.3244, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2393, 0.3200, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2393, 0.3200, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2393, 0.3200, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2890, 0.3150, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2890, 0.3150, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2890, 0.3150, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2890, 0.3150, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1600, 0.2451, 0.5949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1600, 0.2451, 0.5949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1600, 0.2451, 0.5949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2169, 0.2513, 0.5318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2024, 0.2429, 0.5546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1212, 0.1800, 0.6989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2076, 0.2592, 0.5332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2342, 0.2741, 0.4918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3155, 0.3559, 0.3286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2342, 0.2741, 0.4918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2342, 0.2741, 0.4918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2925, 0.3235, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2925, 0.3235, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2925, 0.3235, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2233, 0.2342, 0.5425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2021, 0.2686, 0.5294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2021, 0.2686, 0.5294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0328, 0.0446, 0.9226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0243, 0.0501, 0.9256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0208, 0.0508, 0.9285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0263, 0.0400, 0.9337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0221, 0.0413, 0.9365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0224, 0.0386, 0.9390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0220, 0.0475, 0.9305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0204, 0.0391, 0.9405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0328, 0.0567, 0.9105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0164, 0.0348, 0.9488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0346, 0.9535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0236, 0.0447, 0.9316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0200, 0.0360, 0.9440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0134, 0.0367, 0.9500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0165, 0.0382, 0.9452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0118, 0.0239, 0.9642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0146, 0.0322, 0.9533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0155, 0.0383, 0.9462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0098, 0.0223, 0.9679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0161, 0.0348, 0.9491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0168, 0.0327, 0.9505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0065, 0.0173, 0.9762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0101, 0.0276, 0.9624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0107, 0.0211, 0.9682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0059, 0.0145, 0.9796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0090, 0.0253, 0.9657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0474, 0.0978, 0.8548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0090, 0.0253, 0.9657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0076, 0.0160, 0.9764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0074, 0.0183, 0.9743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0105, 0.0219, 0.9676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0055, 0.0150, 0.9794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0157, 0.0353, 0.9490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0093, 0.0205, 0.9702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0565, 0.1492, 0.7943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0093, 0.0205, 0.9702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0066, 0.0168, 0.9766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0134, 0.0389, 0.9477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0086, 0.0176, 0.9737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0060, 0.0191, 0.9749]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0137, 0.0344, 0.9519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0050, 0.0112, 0.9838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0107, 0.0263, 0.9630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0086, 0.0257, 0.9657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0066, 0.0150, 0.9784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0108, 0.0297, 0.9595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0094, 0.0215, 0.9691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0060, 0.0167, 0.9773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0173, 0.0508, 0.9319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0061, 0.0146, 0.9792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0060, 0.0141, 0.9799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0081, 0.0254, 0.9665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0068, 0.0197, 0.9736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0097, 0.0207, 0.9696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0057, 0.0152, 0.9791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0091, 0.0267, 0.9642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0157, 0.0409, 0.9434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0046, 0.0135, 0.9819]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0063, 0.0149, 0.9787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0079, 0.0256, 0.9666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0038, 0.0112, 0.9849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0038, 0.0100, 0.9862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0039, 0.0091, 0.9871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0018, 0.0079, 0.9903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0046, 0.0140, 0.9814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0030, 0.0080, 0.9890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0026, 0.0072, 0.9902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0099, 0.0340, 0.9561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0028, 0.0090, 0.9882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0037, 0.0103, 0.9860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0044, 0.0119, 0.9837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0018, 0.0076, 0.9906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.0046, 0.9944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0033, 0.0096, 0.9871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0019, 0.0064, 0.9917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0013, 0.0063, 0.9924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.7550e-04, 4.6249e-03, 9.9440e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0016, 0.0065, 0.9920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.1792e-04, 4.1061e-03, 9.9508e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.7805e-04, 3.2616e-03, 9.9616e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.0057, 0.9932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.4938e-04, 3.2787e-03, 9.9617e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.7925e-04, 2.4816e-03, 9.9714e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.9425e-04, 5.3915e-03, 9.9371e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[7.3613e-04, 4.6480e-03, 9.9462e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.4353e-04, 5.0436e-03, 9.9411e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0015, 0.0086, 0.9899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.7635e-04, 6.2188e-03, 9.9280e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0012, 0.0078, 0.9911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "test_args = dict(\n",
    "    nb_sections=3,\n",
    "    proba_change_motor_gain=0,\n",
    "    min_section_length=5,\n",
    "    max_section_length=10,\n",
    "    training=False,\n",
    "    max_episode_steps=250,\n",
    ")\n",
    "env = gym.make('MiniWorld-TaskHallway-v0', view=\"agent\", render_mode=\"human\",**test_args)\n",
    "# env = gym.make('MiniWorld-Hallway-v0', length = 15, view=\"agent\", render_mode=\"human\")\n",
    "env = PyTorchObsWrapper(env)\n",
    "\n",
    "import timeit\n",
    "class CodeTimer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (timeit.default_timer() - self.start) * 1000.0\n",
    "        print('Code block' + self.name + ' took: ' + str(self.took) + ' ms')\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Create the display window\n",
    "env.render()\n",
    "\n",
    "for _ in range(test_args['max_episode_steps']):\n",
    "    action,probs = select_action(observation) # agent policy that uses the observation and info\n",
    "    print(probs)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        print('over')\n",
    "        observation, info = env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m policy\u001b[39m.\u001b[39mvalue_head[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(policy\u001b[39m.\u001b[39mvalue_head[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "policy.value_head[0].weight\n",
    "plt.imshow(policy.value_head[0].weight.cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy.action_head[0].weight\n",
    "plt.imshow(policy.action_head[0].weight.cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c269a8ac1ce32886f1d361a7d7752f19c27b7d8ebd8d0fb0e1cb07b13dddb563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
