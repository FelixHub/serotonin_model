{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import pyglet\n",
    "from pyglet.window import key\n",
    "\n",
    "import miniworld\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "from miniworld.wrappers import PyTorchObsWrapper,GreyscaleWrapper\n",
    "\n",
    "train_args = dict(\n",
    "    nb_sections=3,\n",
    "    proba_change_motor_gain=0,\n",
    "    min_section_length=5,\n",
    "    max_section_length=10,\n",
    "    training=True,\n",
    "    max_episode_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections limits [-1, 4, 9, 18]\n",
      "sections lengths [5, 5, 9]\n",
      "sections motor gains [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('MiniWorld-Hallway-v0', length = 15, view=\"agent\", render_mode=None)\n",
    "env = gym.make('MiniWorld-TaskHallway-v0', view=\"agent\", render_mode=None,**train_args)\n",
    "# env = GreyscaleWrapper(env)\n",
    "env = PyTorchObsWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.hidden_size = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2).to(device)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2).to(device)\n",
    "        self.maxpool = nn.MaxPool2d(2).to(device)\n",
    "\n",
    "        self.outconvsize = 2016\n",
    "        self.affine1 = nn.Linear( self.outconvsize , self.hidden_size).to(device) \n",
    "\n",
    "        ### replacement for rnn \n",
    "        # self.rnn = nn.RNN(self.outconvsize, self.hidden_size,batch_first=True).to(device)\n",
    "        # self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "        self.action_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 3).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 1).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.batch_loss = []\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    # def reset_hidden_state(self):\n",
    "    #     self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.maxpool(self.conv2(x)))\n",
    "        x = x.reshape(-1,self.outconvsize)\n",
    "        out = self.relu(self.affine1(x))\n",
    "        # h = self.rnn(x.unsqueeze(0), self.hidden_state)[1]\n",
    "        # self.hidden_state = h\n",
    "        # out = h.squeeze(0)\n",
    "        action_prob = F.softmax(self.action_head(out), dim=-1)\n",
    "        state_value = self.value_head(out)\n",
    "        return action_prob, state_value\n",
    "\n",
    "\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs,state_value = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append( (m.log_prob(action), state_value) )\n",
    "\n",
    "    return action.item(),probs\n",
    "    \n",
    "gamma = 1\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + eps) # this create error ...\n",
    "    for (log_prob,value), R in zip(policy.saved_log_probs, returns):\n",
    "        advantage = R - value.item()\n",
    "        policy_loss.append(-log_prob * advantage)\n",
    "        value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n",
    "    loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "    policy.batch_loss.append(loss)\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    # policy.reset_hidden_state()\n",
    "\n",
    "\n",
    "def finish_batch():\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy.batch_loss).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.batch_loss[:]\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71876"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need at least 3000 episodes to get a good training\n",
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 100):  # Don't infinite loop while learning\n",
    "            action,show_prob = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            #if args.render:\n",
    "            #    env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 5 == 0:\n",
    "            loss = finish_batch()\n",
    "            print(loss,show_prob.cpu().detach().numpy()[0])\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fphub\\AppData\\Local\\Temp\\ipykernel_13692\\767425537.py:82: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.483146667480469 [0.11410058 0.57820106 0.30769834]\n",
      "Episode 0\tLast reward: 0.00\tAverage reward: 0.00\n",
      "3363.01220703125 [4.0372845e-05 1.6917987e-04 9.9979049e-01]\n",
      "Episode 5\tLast reward: 0.99\tAverage reward: 0.05\n",
      "518.1987915039062 [0.03904464 0.36964428 0.5913111 ]\n",
      "Episode 10\tLast reward: 0.00\tAverage reward: 0.08\n",
      "82.46401977539062 [0.17045425 0.37146878 0.45807692]\n",
      "Episode 15\tLast reward: 0.00\tAverage reward: 0.06\n",
      "719.6245727539062 [0.00509444 0.57933766 0.41556787]\n",
      "Episode 20\tLast reward: 0.00\tAverage reward: 0.05\n",
      "1506.75146484375 [0.01263653 0.04739331 0.9399701 ]\n",
      "Episode 25\tLast reward: 0.00\tAverage reward: 0.04\n",
      "530.50244140625 [0.09020678 0.5247227  0.3850705 ]\n",
      "Episode 30\tLast reward: 0.00\tAverage reward: 0.03\n",
      "-66.27106475830078 [0.48160613 0.48878497 0.02960883]\n",
      "Episode 35\tLast reward: 0.00\tAverage reward: 0.02\n",
      "-74.52922058105469 [0.48559788 0.47432232 0.04007977]\n",
      "Episode 40\tLast reward: 0.00\tAverage reward: 0.02\n",
      "-1.3713698387145996 [0.2540431  0.7142155  0.03174135]\n",
      "Episode 45\tLast reward: 0.00\tAverage reward: 0.01\n",
      "185.28363037109375 [0.59922427 0.3759958  0.02477991]\n",
      "Episode 50\tLast reward: 0.00\tAverage reward: 0.01\n",
      "243.68618774414062 [0.32036054 0.6419111  0.03772834]\n",
      "Episode 55\tLast reward: 0.00\tAverage reward: 0.01\n",
      "277.03045654296875 [0.32570627 0.6444152  0.02987848]\n",
      "Episode 60\tLast reward: 0.00\tAverage reward: 0.01\n",
      "215.4202117919922 [0.22061515 0.73383373 0.04555112]\n",
      "Episode 65\tLast reward: 0.00\tAverage reward: 0.00\n",
      "78.12156677246094 [0.43416154 0.4840755  0.08176304]\n",
      "Episode 70\tLast reward: 0.00\tAverage reward: 0.00\n",
      "-6.998002052307129 [0.66802317 0.30668807 0.02528877]\n",
      "Episode 75\tLast reward: 0.00\tAverage reward: 0.05\n",
      "-81.18746185302734 [0.60238266 0.35195878 0.04565859]\n",
      "Episode 80\tLast reward: 0.00\tAverage reward: 0.04\n",
      "-94.9168701171875 [0.366929   0.6183559  0.01471515]\n",
      "Episode 85\tLast reward: 0.00\tAverage reward: 0.03\n",
      "-96.87158966064453 [0.47617152 0.43136945 0.09245905]\n",
      "Episode 90\tLast reward: 0.00\tAverage reward: 0.02\n",
      "-99.24671936035156 [0.6224622  0.30550483 0.07203288]\n",
      "Episode 95\tLast reward: 0.00\tAverage reward: 0.02\n",
      "-91.96098327636719 [0.4799969  0.34340632 0.17659673]\n",
      "Episode 100\tLast reward: 0.00\tAverage reward: 0.01\n",
      "-71.6710205078125 [0.5854985  0.31073418 0.10376734]\n",
      "Episode 105\tLast reward: 0.00\tAverage reward: 0.01\n",
      "-27.13311767578125 [0.4838866  0.29931265 0.21680082]\n",
      "Episode 110\tLast reward: 0.00\tAverage reward: 0.06\n",
      "-24.5103759765625 [0.43277308 0.38139233 0.1858346 ]\n",
      "Episode 115\tLast reward: 0.00\tAverage reward: 0.04\n",
      "29.08218765258789 [0.4774171  0.31009606 0.21248688]\n",
      "Episode 120\tLast reward: 0.00\tAverage reward: 0.08\n",
      "-3.0764966011047363 [0.45157695 0.41163987 0.13678317]\n",
      "Episode 125\tLast reward: 0.00\tAverage reward: 0.06\n",
      "141.07083129882812 [0.3602152  0.57749903 0.06228578]\n",
      "Episode 130\tLast reward: 0.94\tAverage reward: 0.09\n",
      "19.841524124145508 [0.42469922 0.26154077 0.31376004]\n",
      "Episode 135\tLast reward: 0.00\tAverage reward: 0.11\n",
      "39.10163497924805 [0.40760046 0.42925483 0.16314472]\n",
      "Episode 140\tLast reward: 0.00\tAverage reward: 0.09\n",
      "37.83835983276367 [0.47266206 0.41238174 0.11495615]\n",
      "Episode 145\tLast reward: 0.00\tAverage reward: 0.07\n",
      "178.57534790039062 [0.2760977  0.47085655 0.25304574]\n",
      "Episode 150\tLast reward: 0.93\tAverage reward: 0.10\n",
      "25.203073501586914 [0.35905206 0.4028811  0.23806688]\n",
      "Episode 155\tLast reward: 0.00\tAverage reward: 0.12\n",
      "28.52849578857422 [0.46162415 0.19427878 0.34409702]\n",
      "Episode 160\tLast reward: 0.00\tAverage reward: 0.10\n",
      "17.133453369140625 [0.31078526 0.36738402 0.32183072]\n",
      "Episode 165\tLast reward: 0.00\tAverage reward: 0.07\n",
      "10.567634582519531 [0.33564845 0.35033917 0.31401238]\n",
      "Episode 170\tLast reward: 0.00\tAverage reward: 0.06\n",
      "13.07427978515625 [0.38721904 0.42907128 0.18370967]\n",
      "Episode 175\tLast reward: 0.00\tAverage reward: 0.04\n",
      "14.708099365234375 [0.39415976 0.3682521  0.23758814]\n",
      "Episode 180\tLast reward: 0.00\tAverage reward: 0.07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[82], line 19\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m finish_episode()\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m i_episode \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     loss \u001b[39m=\u001b[39m finish_batch()\n\u001b[0;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(loss,show_prob\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m])\n\u001b[0;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mLast reward: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mAverage reward: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     22\u001b[0m           i_episode, ep_reward, running_reward))\n",
      "Cell \u001b[1;32mIn[80], line 93\u001b[0m, in \u001b[0;36mfinish_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     92\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(policy\u001b[39m.\u001b[39mbatch_loss)\u001b[39m.\u001b[39msum()\n\u001b[1;32m---> 93\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     94\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     95\u001b[0m \u001b[39mdel\u001b[39;00m policy\u001b[39m.\u001b[39mbatch_loss[:]\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'miniworld_task.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Policy:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m policy \u001b[39m=\u001b[39m Policy()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m policy\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mminiworld_001.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Policy:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3])."
     ]
    }
   ],
   "source": [
    "# policy = Policy().to(device)\n",
    "# policy.load_state_dict(torch.load('miniworld_001.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections limits [-1, 4, 13, 19]\n",
      "sections lengths [5, 9, 6]\n",
      "sections motor gains [1, 1, 1]\n",
      "tensor([[0.4503, 0.2810, 0.2687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4443, 0.3058, 0.2499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3934, 0.3925, 0.2141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4443, 0.3058, 0.2499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3169, 0.3376, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3595, 0.4815, 0.1589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3690, 0.3143, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4304, 0.3756, 0.1940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3984, 0.2386, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5694, 0.2144, 0.2162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3984, 0.2386, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5694, 0.2144, 0.2162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3689, 0.2839, 0.3472]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4708, 0.2335, 0.2957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4025, 0.3312, 0.2663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3582, 0.3859, 0.2559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3255, 0.3195, 0.3549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3582, 0.3859, 0.2559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4025, 0.3312, 0.2663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3172, 0.3056, 0.3772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5024, 0.2479, 0.2497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5024, 0.2479, 0.2497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4488, 0.3172, 0.2340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4475, 0.3716, 0.1809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4475, 0.3716, 0.1809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4488, 0.3172, 0.2340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5024, 0.2479, 0.2497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4488, 0.3172, 0.2340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4475, 0.3716, 0.1809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3657, 0.4302, 0.2041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3360, 0.3028, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3360, 0.3028, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3360, 0.3028, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3914, 0.3245, 0.2841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3608, 0.3098, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3914, 0.3245, 0.2841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3608, 0.3098, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3684, 0.3758, 0.2558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4249, 0.2184, 0.3567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3684, 0.3758, 0.2558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3683, 0.4054, 0.2262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3684, 0.3758, 0.2558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3683, 0.4054, 0.2262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3999, 0.3774, 0.2227]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4240, 0.3254, 0.2506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3504, 0.3932, 0.2564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4079, 0.3706, 0.2215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3806, 0.4474, 0.1720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4079, 0.3706, 0.2215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3806, 0.4474, 0.1720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4079, 0.3706, 0.2215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3203, 0.4403, 0.2394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3756, 0.4028, 0.2216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3533, 0.3038, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4631, 0.2999, 0.2370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3533, 0.3038, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4631, 0.2999, 0.2370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3791, 0.4461, 0.1749]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4631, 0.2999, 0.2370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3533, 0.3038, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3534, 0.3797, 0.2669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4277, 0.2777, 0.2946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4755, 0.3152, 0.2093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3644, 0.2784, 0.3572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3423, 0.3135, 0.3442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3948, 0.2934, 0.3119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3382, 0.3527, 0.3091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3948, 0.2934, 0.3119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3932, 0.3448, 0.2621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3473, 0.4058, 0.2470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3378, 0.3404, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3378, 0.3404, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3473, 0.4058, 0.2470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3473, 0.4058, 0.2470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3378, 0.3404, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3378, 0.3404, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3378, 0.3404, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3508, 0.2885, 0.3606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4057, 0.2810, 0.3134]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4080, 0.3604, 0.2315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4760, 0.3317, 0.1923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3467, 0.3261, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4760, 0.3317, 0.1923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3379, 0.3134, 0.3487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4760, 0.3317, 0.1923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3379, 0.3134, 0.3487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3936, 0.3138, 0.2926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4308, 0.2897, 0.2795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3936, 0.3138, 0.2926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3770, 0.3324, 0.2907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3720, 0.3996, 0.2284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3087, 0.3386, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3530, 0.2830, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3530, 0.2830, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3087, 0.3386, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3720, 0.3996, 0.2284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3087, 0.3386, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3087, 0.3386, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3720, 0.3996, 0.2284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3770, 0.3324, 0.2907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MiniWorld-TaskHallway-v0', view=\"agent\", render_mode=\"human\",**train_args)\n",
    "# env = gym.make('MiniWorld-Hallway-v0', length = 15, view=\"agent\", render_mode=\"human\")\n",
    "env = PyTorchObsWrapper(env)\n",
    "\n",
    "import timeit\n",
    "class CodeTimer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (timeit.default_timer() - self.start) * 1000.0\n",
    "        print('Code block' + self.name + ' took: ' + str(self.took) + ' ms')\n",
    "\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Create the display window\n",
    "env.render()\n",
    "\n",
    "for _ in range(100):\n",
    "    action,probs = select_action(observation) # agent policy that uses the observation and info\n",
    "    print(probs)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        print('over')\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Policy' object has no attribute 'rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mimshow(policy\u001b[39m.\u001b[39;49mrnn\u001b[39m.\u001b[39mall_weights[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Policy' object has no attribute 'rnn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(policy.rnn.all_weights[0][0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy.value_head[0].weight\n",
    "plt.imshow(policy.action_head[0].weight.cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 4.6754e-02,  1.5509e-01,  1.4547e-01],\n",
      "          [-2.7186e-02, -1.2516e-01,  1.3034e-02],\n",
      "          [ 1.6854e-02,  5.5227e-03, -7.1415e-02]],\n",
      "\n",
      "         [[ 9.1255e-02, -2.2596e-02,  1.1437e-01],\n",
      "          [ 1.5155e-01, -8.8107e-02, -4.2390e-02],\n",
      "          [-1.5524e-01,  1.0759e-01, -1.7243e-01]],\n",
      "\n",
      "         [[ 1.2472e-01,  1.5281e-01, -1.8184e-01],\n",
      "          [-7.1669e-02,  1.0318e-01, -6.1970e-03],\n",
      "          [-7.7253e-02,  9.8150e-02, -7.3380e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6356e-01,  1.2327e-01, -7.5099e-02],\n",
      "          [-1.0771e-01, -1.4986e-01, -1.0639e-01],\n",
      "          [ 8.5708e-02,  2.8480e-02, -7.0266e-02]],\n",
      "\n",
      "         [[-1.8709e-01, -1.9177e-01, -6.6642e-03],\n",
      "          [-2.4220e-02,  1.5077e-01, -1.6195e-01],\n",
      "          [-1.5576e-01, -2.1827e-02, -1.2727e-01]],\n",
      "\n",
      "         [[ 1.4646e-01,  6.2176e-02,  1.2771e-01],\n",
      "          [-1.6823e-02,  7.7539e-02, -1.6807e-01],\n",
      "          [-6.0254e-02,  1.5988e-01, -1.4931e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.4348e-03, -1.5166e-01,  1.5310e-01],\n",
      "          [-1.6272e-01, -3.6534e-02, -5.0306e-02],\n",
      "          [ 1.5628e-02,  3.3816e-02,  1.4761e-01]],\n",
      "\n",
      "         [[ 1.0994e-01, -6.7447e-02,  9.3045e-02],\n",
      "          [ 1.2917e-01, -1.2394e-02, -1.5191e-01],\n",
      "          [ 1.3876e-01, -1.9789e-01, -1.4797e-01]],\n",
      "\n",
      "         [[-1.4682e-01,  1.4500e-01,  6.0652e-02],\n",
      "          [-1.6894e-01,  1.7886e-01,  1.3258e-01],\n",
      "          [-4.3454e-02,  1.3055e-02, -1.3945e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.8594e-01,  9.1385e-02, -3.7175e-02],\n",
      "          [-1.2260e-01,  1.2459e-01, -1.4757e-01],\n",
      "          [ 9.5826e-02,  1.6181e-01,  9.8321e-02]],\n",
      "\n",
      "         [[ 1.7039e-01,  8.2151e-02, -1.1882e-01],\n",
      "          [ 6.6750e-02,  7.2834e-03,  1.3372e-01],\n",
      "          [ 9.2426e-02,  5.5486e-02, -3.2698e-02]],\n",
      "\n",
      "         [[-7.6420e-02,  1.5975e-01,  2.7319e-02],\n",
      "          [-1.6656e-01, -7.4352e-02,  9.3445e-02],\n",
      "          [ 1.6969e-01,  1.4261e-01,  3.4700e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.1910e-02, -1.2236e-01,  1.2530e-01],\n",
      "          [ 1.0944e-01, -6.5109e-03,  9.7749e-03],\n",
      "          [ 1.6387e-01, -1.7729e-01, -1.5541e-01]],\n",
      "\n",
      "         [[ 5.0026e-02, -1.0231e-01,  2.2259e-02],\n",
      "          [-6.6220e-02, -8.9394e-02,  1.6667e-01],\n",
      "          [ 5.4384e-02, -1.6049e-01,  1.1029e-02]],\n",
      "\n",
      "         [[-9.6837e-02, -1.7009e-01,  1.4368e-01],\n",
      "          [-7.1629e-02,  4.3055e-02,  1.1015e-01],\n",
      "          [ 5.7986e-02, -4.4209e-02, -6.4074e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5935e-01,  2.6692e-02,  1.0669e-01],\n",
      "          [ 1.0403e-01, -9.9551e-02,  1.0867e-01],\n",
      "          [ 1.5423e-01, -1.0801e-01, -4.8240e-03]],\n",
      "\n",
      "         [[ 6.0529e-02,  2.2527e-02,  6.4974e-02],\n",
      "          [ 9.3466e-02,  1.2069e-01, -1.0524e-03],\n",
      "          [ 1.6033e-01, -1.3771e-01,  6.8947e-02]],\n",
      "\n",
      "         [[ 1.0404e-03, -9.8720e-02, -1.5632e-01],\n",
      "          [-2.5014e-02, -1.8170e-01, -7.1575e-02],\n",
      "          [-1.0621e-01, -1.2119e-01, -8.8695e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8889e-01, -1.8981e-01,  5.7160e-02],\n",
      "          [ 9.5623e-03, -1.7102e-01, -1.7608e-01],\n",
      "          [ 1.7591e-01,  1.0011e-01,  6.7892e-02]],\n",
      "\n",
      "         [[-2.2160e-02,  2.6944e-02, -3.7456e-02],\n",
      "          [-8.8131e-02, -1.5175e-01,  4.6450e-02],\n",
      "          [ 1.4387e-01,  1.3953e-01, -1.5489e-01]],\n",
      "\n",
      "         [[ 1.8219e-01, -3.7140e-03,  1.1091e-01],\n",
      "          [-4.4001e-02, -1.0384e-01,  4.0394e-02],\n",
      "          [ 6.7989e-02, -1.9183e-01,  4.9794e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3728e-02, -2.0861e-01, -1.8652e-01],\n",
      "          [ 1.6200e-01,  9.7898e-02,  1.1848e-01],\n",
      "          [-1.1451e-01,  8.2955e-02,  1.6502e-01]],\n",
      "\n",
      "         [[ 9.5818e-02,  1.3770e-02,  4.2829e-02],\n",
      "          [-1.8668e-01,  4.9457e-02,  7.4229e-02],\n",
      "          [-9.3412e-02, -2.2750e-02,  4.0804e-02]],\n",
      "\n",
      "         [[-4.0047e-03,  1.7306e-01,  1.0643e-01],\n",
      "          [-2.0470e-01, -7.4672e-02, -9.8419e-02],\n",
      "          [-1.4014e-01,  1.1301e-01,  1.6482e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.9749e-01, -1.4059e-01,  8.0962e-02],\n",
      "          [-6.2520e-02,  1.0109e-01, -3.7690e-02],\n",
      "          [ 1.0475e-01,  1.3596e-01,  4.5945e-02]],\n",
      "\n",
      "         [[ 1.7088e-01, -1.9517e-01,  4.7274e-02],\n",
      "          [ 1.1095e-01, -4.0374e-02,  1.4410e-01],\n",
      "          [-1.0774e-01,  1.0786e-01,  1.0696e-01]],\n",
      "\n",
      "         [[-4.4577e-02,  1.2307e-01,  5.5013e-02],\n",
      "          [ 1.0035e-01, -1.8913e-01, -2.6963e-02],\n",
      "          [ 2.9936e-02,  6.2252e-02,  1.6714e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3595e-02, -9.8850e-02, -7.3839e-03],\n",
      "          [ 2.8559e-02,  6.1903e-02, -1.1355e-01],\n",
      "          [ 3.2005e-02, -4.9124e-02,  1.8222e-05]],\n",
      "\n",
      "         [[ 1.8915e-02,  1.4422e-01, -3.6064e-02],\n",
      "          [ 1.8858e-02,  9.6794e-02,  1.1933e-01],\n",
      "          [ 7.8523e-02,  8.3428e-02,  1.2462e-01]],\n",
      "\n",
      "         [[ 1.8421e-01, -1.4491e-01,  2.8371e-02],\n",
      "          [ 7.2898e-02, -1.2212e-01,  3.0349e-02],\n",
      "          [ 5.7633e-02, -1.8963e-01,  1.3371e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8232e-01, -1.1576e-01,  9.5966e-02],\n",
      "          [-1.7955e-01,  2.3869e-03, -1.8987e-01],\n",
      "          [ 7.0672e-02,  1.5051e-01, -5.9943e-02]],\n",
      "\n",
      "         [[ 1.4910e-01, -7.3903e-02,  9.1931e-02],\n",
      "          [ 1.0390e-01, -8.3008e-02, -1.0648e-01],\n",
      "          [ 1.7957e-01, -1.1503e-01,  1.0164e-01]],\n",
      "\n",
      "         [[ 7.4326e-02,  2.5027e-02,  6.6159e-02],\n",
      "          [-1.3053e-01,  3.1277e-02, -2.1803e-02],\n",
      "          [ 1.2160e-01, -3.3432e-02, -1.1916e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6045e-01, -5.2711e-02, -1.4663e-01],\n",
      "          [-8.7701e-02, -9.2601e-02,  6.0791e-02],\n",
      "          [ 2.4178e-02,  5.9969e-02, -3.5660e-02]],\n",
      "\n",
      "         [[-8.7826e-02, -2.4632e-02,  1.2184e-01],\n",
      "          [-4.5932e-02,  1.3831e-01, -6.7216e-02],\n",
      "          [-1.7573e-01, -1.7468e-02,  1.6972e-01]],\n",
      "\n",
      "         [[ 1.0578e-01,  6.8136e-02,  1.7176e-02],\n",
      "          [-1.2548e-01,  9.4512e-02, -2.3414e-02],\n",
      "          [-1.5196e-01, -1.3888e-01, -5.3968e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.8757e-02,  3.4095e-02,  1.4896e-02],\n",
      "          [ 8.5739e-02, -1.4840e-01,  1.4602e-01],\n",
      "          [-1.1289e-01,  6.3821e-02,  1.6038e-01]],\n",
      "\n",
      "         [[ 2.1940e-02, -1.0484e-01,  9.9088e-02],\n",
      "          [-1.6399e-01, -1.4789e-01,  1.2206e-01],\n",
      "          [-2.1134e-01, -1.9217e-01,  3.6054e-02]],\n",
      "\n",
      "         [[-1.6319e-01, -3.2560e-02, -1.7257e-01],\n",
      "          [-1.6115e-01, -1.2015e-01, -9.9715e-02],\n",
      "          [-2.0709e-01, -4.0498e-02, -8.8335e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.6821e-02, -2.3788e-02,  1.0590e-01],\n",
      "          [-1.0749e-01,  3.0846e-02,  6.4930e-02],\n",
      "          [ 3.3891e-02,  1.0056e-01,  6.1576e-02]],\n",
      "\n",
      "         [[-1.1954e-01,  3.5335e-02, -5.7185e-02],\n",
      "          [-1.8316e-01, -1.8696e-01,  1.3237e-01],\n",
      "          [ 1.2640e-01,  1.2945e-01, -1.2571e-01]],\n",
      "\n",
      "         [[-7.2172e-02,  9.2532e-02, -1.4226e-01],\n",
      "          [ 1.6530e-01,  5.9929e-02, -1.4009e-01],\n",
      "          [ 5.7612e-02, -1.7954e-01,  5.5217e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.7680e-02, -9.4403e-02,  1.1391e-01],\n",
      "          [ 9.6448e-02, -2.3900e-02,  2.7736e-03],\n",
      "          [ 7.9322e-02, -1.5317e-01,  1.5694e-01]],\n",
      "\n",
      "         [[ 7.5870e-02, -1.8154e-01, -1.8036e-01],\n",
      "          [-7.2122e-02,  1.2285e-02,  1.3862e-01],\n",
      "          [-1.0982e-01, -2.9703e-02,  9.8934e-02]],\n",
      "\n",
      "         [[ 4.9308e-02, -1.1929e-02, -6.5760e-02],\n",
      "          [ 4.7046e-02,  6.7523e-02, -1.0226e-01],\n",
      "          [-1.5349e-01, -7.5551e-02,  1.2734e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.4646e-02,  1.7817e-01, -1.7041e-01],\n",
      "          [-1.2982e-01, -9.1340e-02, -1.2026e-01],\n",
      "          [-1.8958e-01, -8.5426e-02,  1.2844e-01]],\n",
      "\n",
      "         [[-1.4524e-01,  1.8342e-01, -6.6860e-02],\n",
      "          [-2.2393e-02,  6.4164e-03,  1.2929e-01],\n",
      "          [ 2.5928e-02, -6.2904e-03, -9.5353e-02]],\n",
      "\n",
      "         [[-2.6931e-02, -1.7365e-02, -1.2194e-01],\n",
      "          [-1.8340e-01, -2.1609e-02, -1.0161e-01],\n",
      "          [-2.0012e-01,  1.6984e-01,  7.4336e-02]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "conv1.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0496, -0.1046,  0.1314, -0.0383,  0.0139, -0.0945,  0.0742, -0.0750,\n",
      "         0.0233, -0.1170,  0.0050, -0.0645, -0.0129,  0.1551,  0.0950,  0.1237],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "conv2.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0630,  0.0106, -0.0012],\n",
      "          [-0.0444, -0.0779,  0.0301],\n",
      "          [-0.0861, -0.0673, -0.0353]],\n",
      "\n",
      "         [[-0.0536, -0.0721, -0.0674],\n",
      "          [-0.0650,  0.0807,  0.0091],\n",
      "          [ 0.0166,  0.0157, -0.0670]],\n",
      "\n",
      "         [[-0.0286, -0.0314, -0.0181],\n",
      "          [ 0.0652,  0.0439,  0.0666],\n",
      "          [-0.0537, -0.0876,  0.0577]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0574,  0.0741,  0.0132],\n",
      "          [-0.0083,  0.0106, -0.0560],\n",
      "          [ 0.0602, -0.0345, -0.0488]],\n",
      "\n",
      "         [[-0.0219, -0.0403, -0.0798],\n",
      "          [-0.0220, -0.0164,  0.0422],\n",
      "          [-0.0022,  0.0656, -0.0208]],\n",
      "\n",
      "         [[ 0.0568, -0.0606,  0.0428],\n",
      "          [ 0.0188, -0.0474,  0.1094],\n",
      "          [ 0.0257,  0.0117,  0.0257]]],\n",
      "\n",
      "\n",
      "        [[[-0.0345, -0.0623, -0.0038],\n",
      "          [-0.0354, -0.0085, -0.0729],\n",
      "          [-0.0684, -0.0570, -0.0491]],\n",
      "\n",
      "         [[-0.0008, -0.0192, -0.0008],\n",
      "          [ 0.0454, -0.0300, -0.0214],\n",
      "          [-0.0067,  0.0232, -0.0388]],\n",
      "\n",
      "         [[ 0.0086,  0.0028, -0.0627],\n",
      "          [ 0.0893, -0.0453,  0.0141],\n",
      "          [-0.0203,  0.0221,  0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0803,  0.0070,  0.0422],\n",
      "          [-0.0115, -0.0492, -0.0157],\n",
      "          [ 0.0315, -0.0938,  0.0239]],\n",
      "\n",
      "         [[ 0.0125,  0.0476, -0.0353],\n",
      "          [ 0.0508, -0.0687, -0.0574],\n",
      "          [-0.0159, -0.0727, -0.0681]],\n",
      "\n",
      "         [[-0.0444, -0.0384,  0.0084],\n",
      "          [ 0.0658, -0.0261, -0.0449],\n",
      "          [-0.0868, -0.0008, -0.0904]]],\n",
      "\n",
      "\n",
      "        [[[-0.0481,  0.0554, -0.0111],\n",
      "          [ 0.0258, -0.0812, -0.0780],\n",
      "          [-0.0738,  0.0567, -0.0761]],\n",
      "\n",
      "         [[ 0.0749, -0.0863,  0.0460],\n",
      "          [-0.0032,  0.0431,  0.0714],\n",
      "          [-0.0411,  0.0130, -0.0155]],\n",
      "\n",
      "         [[-0.0584, -0.0284, -0.0824],\n",
      "          [ 0.0554,  0.0241,  0.0572],\n",
      "          [-0.0326, -0.0032, -0.0898]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0161,  0.0060, -0.0229],\n",
      "          [ 0.0857,  0.0301, -0.0632],\n",
      "          [-0.0280,  0.0081,  0.0221]],\n",
      "\n",
      "         [[ 0.0175, -0.0782, -0.0831],\n",
      "          [-0.0133, -0.0698, -0.0484],\n",
      "          [ 0.0554, -0.0317, -0.0268]],\n",
      "\n",
      "         [[-0.0890, -0.0839,  0.0028],\n",
      "          [ 0.0309,  0.0334, -0.0188],\n",
      "          [-0.0254,  0.0435, -0.0728]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0182, -0.0207, -0.0354],\n",
      "          [-0.0733, -0.0577, -0.0120],\n",
      "          [ 0.0612, -0.0635, -0.0209]],\n",
      "\n",
      "         [[-0.0519, -0.0537,  0.0222],\n",
      "          [ 0.0560,  0.0591,  0.0165],\n",
      "          [-0.0827,  0.0232, -0.0498]],\n",
      "\n",
      "         [[-0.0032, -0.0462, -0.0769],\n",
      "          [ 0.0089, -0.0132, -0.0014],\n",
      "          [-0.0285, -0.0578, -0.0130]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0232, -0.0870,  0.0615],\n",
      "          [ 0.0067,  0.0215,  0.0251],\n",
      "          [-0.0886, -0.0046,  0.0608]],\n",
      "\n",
      "         [[-0.0594,  0.0292,  0.0462],\n",
      "          [-0.0319,  0.0104, -0.0518],\n",
      "          [-0.0423, -0.0577,  0.0478]],\n",
      "\n",
      "         [[-0.0174,  0.0171, -0.0048],\n",
      "          [-0.0134, -0.0223,  0.0964],\n",
      "          [-0.0129, -0.0207,  0.0862]]],\n",
      "\n",
      "\n",
      "        [[[-0.0558, -0.0844,  0.0175],\n",
      "          [-0.0312,  0.0170, -0.0850],\n",
      "          [-0.0745,  0.0059, -0.0358]],\n",
      "\n",
      "         [[-0.0125, -0.0462, -0.0423],\n",
      "          [-0.0828,  0.0215,  0.0360],\n",
      "          [ 0.0051,  0.0010, -0.0216]],\n",
      "\n",
      "         [[-0.0448,  0.0125, -0.0344],\n",
      "          [-0.0474, -0.0761,  0.0304],\n",
      "          [-0.0237, -0.0831,  0.0168]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0724,  0.0649, -0.0176],\n",
      "          [ 0.0567, -0.0463,  0.0044],\n",
      "          [-0.0003, -0.0077, -0.0309]],\n",
      "\n",
      "         [[-0.0544,  0.0730,  0.0324],\n",
      "          [-0.0476, -0.0715,  0.0116],\n",
      "          [-0.0354, -0.0805,  0.0325]],\n",
      "\n",
      "         [[-0.0433,  0.0410,  0.0093],\n",
      "          [ 0.0173, -0.0981, -0.1121],\n",
      "          [-0.0587, -0.0220,  0.0504]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0378,  0.0601,  0.0132],\n",
      "          [ 0.0650,  0.0718,  0.0075],\n",
      "          [-0.0825, -0.0805, -0.0274]],\n",
      "\n",
      "         [[-0.0406, -0.0043, -0.0248],\n",
      "          [-0.0026, -0.0342,  0.0284],\n",
      "          [-0.0349,  0.0183,  0.0665]],\n",
      "\n",
      "         [[-0.0584, -0.0269, -0.0198],\n",
      "          [ 0.0579, -0.0186, -0.0729],\n",
      "          [-0.0198, -0.0043, -0.0036]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0441, -0.0695, -0.0224],\n",
      "          [ 0.0673,  0.0513,  0.0134],\n",
      "          [ 0.0276, -0.0378, -0.0522]],\n",
      "\n",
      "         [[ 0.0044,  0.0692, -0.0826],\n",
      "          [ 0.0152, -0.0650,  0.0379],\n",
      "          [ 0.0625,  0.0676,  0.0604]],\n",
      "\n",
      "         [[ 0.0566, -0.0685,  0.0078],\n",
      "          [ 0.0187, -0.0537, -0.0890],\n",
      "          [ 0.0587, -0.0731,  0.0503]]]], device='cuda:0', requires_grad=True)\n",
      "conv2.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0865, -0.0538,  0.0475, -0.0523, -0.0516, -0.0455, -0.0890,  0.0685,\n",
      "         0.0461, -0.0522,  0.0579,  0.1010,  0.0225, -0.0029,  0.0364, -0.0271,\n",
      "         0.0768, -0.0271, -0.0381,  0.0366, -0.0136, -0.0828, -0.0741, -0.0551,\n",
      "         0.0875, -0.0748,  0.0427, -0.0330,  0.0664,  0.0935, -0.0801, -0.0801],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "affine1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0142,  0.0015,  0.0047,  ...,  0.0178,  0.0023, -0.0179],\n",
      "        [ 0.0084, -0.0010,  0.0112,  ..., -0.0162,  0.0027, -0.0150],\n",
      "        [-0.0085,  0.0075,  0.0025,  ..., -0.0176,  0.0065,  0.0025],\n",
      "        ...,\n",
      "        [-0.0089,  0.0167,  0.0026,  ...,  0.0058, -0.0139,  0.0137],\n",
      "        [ 0.0121, -0.0144, -0.0095,  ..., -0.0002, -0.0010, -0.0075],\n",
      "        [-0.0236, -0.0228,  0.0027,  ...,  0.0007, -0.0007,  0.0150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "affine1.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0168,  0.0014, -0.0234,  0.0328, -0.0115,  0.0050,  0.0028, -0.0162,\n",
      "         0.0017, -0.0040, -0.0077,  0.0029, -0.0026, -0.0216, -0.0143, -0.0152,\n",
      "         0.0050, -0.0043, -0.0095, -0.0041, -0.0010, -0.0118,  0.0089, -0.0121,\n",
      "        -0.0026,  0.0026,  0.0011,  0.0018, -0.0205, -0.0141,  0.0122, -0.0173],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "action_head.0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1110, -0.0256,  0.1488,  ..., -0.1631, -0.0245, -0.0525],\n",
      "        [-0.0576, -0.1350,  0.1565,  ...,  0.0500,  0.0326, -0.1709],\n",
      "        [ 0.0782,  0.0422, -0.1587,  ...,  0.1372,  0.1444, -0.0939],\n",
      "        ...,\n",
      "        [-0.1761,  0.0131,  0.1094,  ...,  0.0007,  0.0319, -0.1552],\n",
      "        [-0.0146, -0.0366,  0.1192,  ...,  0.1030, -0.1421,  0.0554],\n",
      "        [-0.1107, -0.1417, -0.0806,  ..., -0.0847, -0.0773,  0.0142]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "action_head.0.bias\n",
      "Parameter containing:\n",
      "tensor([-0.1389,  0.0523, -0.0155,  0.1209, -0.1764,  0.0025,  0.0693,  0.0296,\n",
      "         0.1452,  0.0822, -0.1668,  0.1016,  0.0856, -0.1545, -0.1627, -0.0291,\n",
      "        -0.1855,  0.1267,  0.1668,  0.1372, -0.1823, -0.0540,  0.0681,  0.1631,\n",
      "         0.0146,  0.1308, -0.0944,  0.1565, -0.0707, -0.0500, -0.0600,  0.1313],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "action_head.2.weight\n",
      "Parameter containing:\n",
      "tensor([[ 6.9978e-02, -1.4122e-01,  7.8310e-02, -9.8323e-02, -1.3625e-01,\n",
      "         -1.3112e-01,  1.3812e-01,  5.0476e-02, -1.5376e-01, -7.3748e-02,\n",
      "          4.6892e-02, -8.4407e-02, -3.5012e-02, -7.3176e-02,  7.0460e-02,\n",
      "          8.6945e-02, -1.6590e-02, -5.8034e-02,  1.2388e-01, -5.2516e-03,\n",
      "         -1.2797e-01,  1.3652e-01, -1.0192e-01, -7.2340e-02,  5.0386e-02,\n",
      "         -1.2830e-01,  1.3144e-01,  1.5540e-01,  9.3362e-02,  1.4281e-01,\n",
      "         -5.7134e-02, -2.2411e-02],\n",
      "        [ 1.3444e-04, -4.2597e-02, -4.8117e-02,  1.4144e-01, -4.5257e-02,\n",
      "          1.4651e-01, -1.5221e-01,  9.8104e-02,  1.5112e-01,  1.2057e-01,\n",
      "         -9.8334e-02,  4.4386e-02,  4.0052e-02,  1.4059e-01,  1.3977e-01,\n",
      "          6.1771e-02, -1.4354e-01,  7.8452e-02,  1.7999e-02, -5.3746e-02,\n",
      "         -1.0552e-01,  7.6510e-02, -1.2800e-01, -6.5440e-02, -7.2763e-02,\n",
      "         -5.8363e-02, -4.5092e-02,  1.0193e-02, -2.8129e-02, -2.4930e-02,\n",
      "         -7.2946e-02, -8.2058e-02],\n",
      "        [ 1.3371e-01, -1.0604e-01, -3.3889e-02,  9.6280e-02, -3.7767e-02,\n",
      "          4.9406e-03, -1.9324e-02,  9.0705e-02, -7.9297e-02, -5.0295e-02,\n",
      "         -7.1067e-02, -4.6265e-02, -9.9478e-02,  1.3367e-01, -1.4809e-01,\n",
      "          1.0251e-01, -1.5888e-01, -2.9063e-02,  1.0718e-01, -1.0517e-01,\n",
      "          1.4884e-01, -1.2200e-01, -3.1417e-02, -1.1627e-01, -1.3747e-01,\n",
      "         -9.7987e-02,  1.2377e-01, -5.9511e-03, -1.2845e-01, -3.1282e-02,\n",
      "         -7.6085e-02, -6.4416e-02]], device='cuda:0', requires_grad=True)\n",
      "action_head.2.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0638,  0.0797, -0.0281], device='cuda:0', requires_grad=True)\n",
      "value_head.0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1429, -0.1614,  0.0989,  ..., -0.1761,  0.0829,  0.0383],\n",
      "        [-0.0536, -0.1227,  0.0479,  ..., -0.1095, -0.1033,  0.1434],\n",
      "        [ 0.1141, -0.0325, -0.1667,  ...,  0.1431,  0.0388,  0.0856],\n",
      "        ...,\n",
      "        [ 0.1550,  0.1018,  0.0706,  ...,  0.0211, -0.0045, -0.1026],\n",
      "        [ 0.1152, -0.0708,  0.0192,  ...,  0.1361, -0.0243,  0.0731],\n",
      "        [-0.1323, -0.0567,  0.0003,  ...,  0.0522, -0.1468,  0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "value_head.0.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0236, -0.1676, -0.1035, -0.1275, -0.0422, -0.1496,  0.0509,  0.2333,\n",
      "        -0.1179, -0.0636,  0.0088,  0.0695, -0.0551,  0.0885, -0.0248,  0.1269,\n",
      "        -0.0091, -0.0333,  0.0075,  0.0559,  0.0883, -0.0258, -0.0614,  0.1054,\n",
      "        -0.1346, -0.0874,  0.1799,  0.0241, -0.1136,  0.1272,  0.0334,  0.0463],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "value_head.2.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1592, -0.0784,  0.0011, -0.0705, -0.1153,  0.0779, -0.1610,  0.1238,\n",
      "         -0.1130, -0.0809, -0.1182,  0.0634,  0.0678,  0.0280,  0.0593,  0.0006,\n",
      "         -0.0221, -0.0049, -0.1413, -0.1222,  0.0096,  0.0932,  0.1416, -0.0152,\n",
      "         -0.1032, -0.0120,  0.0030, -0.0292, -0.0023, -0.0851, -0.1112,  0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "value_head.2.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0330], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,part in policy.named_parameters():\n",
    "    print(name)\n",
    "    print(part)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c269a8ac1ce32886f1d361a7d7752f19c27b7d8ebd8d0fb0e1cb07b13dddb563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
