{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import pyglet\n",
    "from pyglet.window import key\n",
    "\n",
    "import miniworld\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "from miniworld.wrappers import PyTorchObsWrapper,GreyscaleWrapper\n",
    "\n",
    "train_args = dict(\n",
    "    nb_sections=2,\n",
    "    proba_change_motor_gain=0,\n",
    "    min_section_length=5,\n",
    "    max_section_length=10,\n",
    "    training=True,\n",
    "    max_episode_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections limits [-1, 5, 11]\n",
      "sections lengths [6, 6]\n",
      "sections motor gains [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('MiniWorld-Hallway-v0', length = 15, view=\"agent\", render_mode=None)\n",
    "env = gym.make('MiniWorld-TaskHallway-v0', view=\"agent\", render_mode=None,**train_args)\n",
    "# env = GreyscaleWrapper(env)\n",
    "env = PyTorchObsWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.hidden_size = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2).to(device)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2).to(device)\n",
    "        self.maxpool = nn.MaxPool2d(2).to(device)\n",
    "\n",
    "        self.outconvsize = 2016\n",
    "        self.affine1 = nn.Linear( self.outconvsize , self.hidden_size).to(device) \n",
    "\n",
    "        ### replacement for rnn \n",
    "        # self.rnn = nn.RNN(self.outconvsize, self.hidden_size,batch_first=True).to(device)\n",
    "        # self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "        self.action_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 3).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 1).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.batch_loss = []\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    # def reset_hidden_state(self):\n",
    "    #     self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.maxpool(self.conv2(x)))\n",
    "        x = x.reshape(-1,self.outconvsize)\n",
    "        out = self.relu(self.affine1(x))\n",
    "        # h = self.rnn(x.unsqueeze(0), self.hidden_state)[1]\n",
    "        # self.hidden_state = h\n",
    "        # out = h.squeeze(0)\n",
    "        action_prob = F.softmax(self.action_head(out), dim=-1)\n",
    "        state_value = self.value_head(out)\n",
    "        return action_prob, state_value\n",
    "\n",
    "\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs,state_value = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append( (m.log_prob(action), state_value) )\n",
    "\n",
    "    return action.item(),probs\n",
    "    \n",
    "gamma = 1\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + eps) # this create error ...\n",
    "    for (log_prob,value), R in zip(policy.saved_log_probs, returns):\n",
    "        advantage = R - value.item()\n",
    "        policy_loss.append(-log_prob * advantage)\n",
    "        value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n",
    "    loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "    policy.batch_loss.append(loss)\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    # policy.reset_hidden_state()\n",
    "\n",
    "\n",
    "def finish_batch():\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy.batch_loss).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.batch_loss[:]\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71876"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need at least 3000 episodes to get a good training\n",
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 100):  # Don't infinite loop while learning\n",
    "            action,show_prob = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            #if args.render:\n",
    "            #    env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 5 == 0:\n",
    "            loss = finish_batch()\n",
    "            print(loss,show_prob.cpu().detach().numpy()[0])\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1880919/767425537.py:82: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.784473419189453 [0.0038987  0.02190608 0.97419524]\n",
      "Episode 0\tLast reward: 0.97\tAverage reward: 0.05\n",
      "-12.62089729309082 [8.7912614e-04 6.0491942e-02 9.3862891e-01]\n",
      "Episode 5\tLast reward: 0.98\tAverage reward: 0.21\n",
      "-24.93584442138672 [2.1144362e-04 3.4537882e-02 9.6525061e-01]\n",
      "Episode 10\tLast reward: 0.97\tAverage reward: 0.30\n",
      "-33.18297576904297 [0.00497434 0.17542477 0.81960094]\n",
      "Episode 15\tLast reward: 0.00\tAverage reward: 0.36\n",
      "-63.24586486816406 [0.01682576 0.34381014 0.6393641 ]\n",
      "Episode 20\tLast reward: 0.00\tAverage reward: 0.33\n",
      "2.962677001953125 [0.00190617 0.10084528 0.8972485 ]\n",
      "Episode 25\tLast reward: 0.98\tAverage reward: 0.39\n",
      "-68.63056182861328 [0.00433232 0.25704485 0.73862284]\n",
      "Episode 30\tLast reward: 0.00\tAverage reward: 0.34\n",
      "-1.3374576568603516 [0.00127798 0.11258892 0.8861331 ]\n",
      "Episode 35\tLast reward: 0.93\tAverage reward: 0.31\n",
      "2.373098373413086 [0.01148186 0.23062769 0.7578905 ]\n",
      "Episode 40\tLast reward: 0.94\tAverage reward: 0.29\n",
      "-21.629749298095703 [0.1825776  0.73537564 0.0820467 ]\n",
      "Episode 45\tLast reward: 0.00\tAverage reward: 0.26\n",
      "-57.53586196899414 [0.24463134 0.5121144  0.24325423]\n",
      "Episode 50\tLast reward: 0.00\tAverage reward: 0.20\n",
      "-44.849098205566406 [0.23052604 0.6770287  0.09244521]\n",
      "Episode 55\tLast reward: 0.00\tAverage reward: 0.20\n",
      "-37.94889450073242 [0.02713825 0.16566522 0.80719656]\n",
      "Episode 60\tLast reward: 0.00\tAverage reward: 0.19\n",
      "-6.924075126647949 [0.33020523 0.40414718 0.26564762]\n",
      "Episode 65\tLast reward: 0.00\tAverage reward: 0.24\n",
      "-31.123355865478516 [0.02098853 0.24845205 0.7305594 ]\n",
      "Episode 70\tLast reward: 1.00\tAverage reward: 0.27\n",
      "-30.97197151184082 [0.3619835  0.47777048 0.160246  ]\n",
      "Episode 75\tLast reward: 0.00\tAverage reward: 0.26\n",
      "-8.089728355407715 [0.33522892 0.43700498 0.22776611]\n",
      "Episode 80\tLast reward: 0.00\tAverage reward: 0.28\n",
      "108.54666137695312 [0.18685463 0.37028965 0.4428557 ]\n",
      "Episode 85\tLast reward: 0.98\tAverage reward: 0.31\n",
      "135.23963928222656 [0.41170484 0.39217514 0.19612005]\n",
      "Episode 90\tLast reward: 0.00\tAverage reward: 0.37\n",
      "86.73070526123047 [0.36375326 0.41065472 0.225592  ]\n",
      "Episode 95\tLast reward: 0.00\tAverage reward: 0.37\n",
      "111.56669616699219 [0.33874455 0.3778719  0.2833836 ]\n",
      "Episode 100\tLast reward: 0.00\tAverage reward: 0.37\n",
      "58.434783935546875 [0.04278721 0.18848455 0.7687282 ]\n",
      "Episode 105\tLast reward: 0.00\tAverage reward: 0.37\n",
      "88.07650756835938 [0.13208772 0.25306958 0.6148427 ]\n",
      "Episode 110\tLast reward: 0.95\tAverage reward: 0.47\n",
      "44.68385314941406 [0.03914209 0.1758095  0.7850484 ]\n",
      "Episode 115\tLast reward: 0.00\tAverage reward: 0.40\n",
      "-2.108649253845215 [0.34124306 0.33350542 0.3252515 ]\n",
      "Episode 120\tLast reward: 0.00\tAverage reward: 0.40\n",
      "-7.778687477111816 [0.02488505 0.12036099 0.85475403]\n",
      "Episode 125\tLast reward: 1.00\tAverage reward: 0.40\n",
      "-35.316314697265625 [0.3629     0.10697152 0.5301285 ]\n",
      "Episode 130\tLast reward: 0.00\tAverage reward: 0.31\n",
      "13.01913070678711 [0.45091218 0.30328608 0.24580166]\n",
      "Episode 135\tLast reward: 0.00\tAverage reward: 0.32\n",
      "4.467802047729492 [0.07302545 0.12500882 0.8019657 ]\n",
      "Episode 140\tLast reward: 0.99\tAverage reward: 0.34\n",
      "49.29442596435547 [0.13128826 0.21063212 0.6580796 ]\n",
      "Episode 145\tLast reward: 0.95\tAverage reward: 0.40\n",
      "121.23755645751953 [0.02129179 0.08486548 0.8938427 ]\n",
      "Episode 150\tLast reward: 0.99\tAverage reward: 0.49\n",
      "47.606117248535156 [0.03432574 0.11034022 0.8553341 ]\n",
      "Episode 155\tLast reward: 0.99\tAverage reward: 0.47\n",
      "50.66890335083008 [0.27702138 0.26006827 0.46291035]\n",
      "Episode 160\tLast reward: 0.00\tAverage reward: 0.49\n",
      "49.80501174926758 [0.01960112 0.04636681 0.934032  ]\n",
      "Episode 165\tLast reward: 0.00\tAverage reward: 0.51\n",
      "80.87005615234375 [0.03837074 0.10858823 0.853041  ]\n",
      "Episode 170\tLast reward: 0.99\tAverage reward: 0.53\n",
      "24.219533920288086 [0.28331977 0.21837406 0.49830607]\n",
      "Episode 175\tLast reward: 0.99\tAverage reward: 0.59\n",
      "71.35990905761719 [0.01005388 0.04247127 0.9474748 ]\n",
      "Episode 180\tLast reward: 0.97\tAverage reward: 0.59\n",
      "21.107574462890625 [0.1248042  0.08580887 0.789387  ]\n",
      "Episode 185\tLast reward: 0.97\tAverage reward: 0.59\n",
      "62.15653610229492 [0.31374815 0.29248896 0.39376298]\n",
      "Episode 190\tLast reward: 0.00\tAverage reward: 0.54\n",
      "-9.136409759521484 [0.10426097 0.13671996 0.75901914]\n",
      "Episode 195\tLast reward: 0.00\tAverage reward: 0.55\n",
      "32.105064392089844 [0.33499447 0.2612689  0.4037366 ]\n",
      "Episode 200\tLast reward: 0.00\tAverage reward: 0.55\n",
      "27.523517608642578 [0.00617258 0.03119612 0.9626313 ]\n",
      "Episode 205\tLast reward: 0.93\tAverage reward: 0.56\n",
      "47.61848068237305 [0.26707104 0.21340115 0.5195278 ]\n",
      "Episode 210\tLast reward: 0.99\tAverage reward: 0.62\n",
      "62.015235900878906 [0.00233564 0.00775635 0.989908  ]\n",
      "Episode 215\tLast reward: 0.00\tAverage reward: 0.60\n",
      "26.39983367919922 [0.00526357 0.02772142 0.967015  ]\n",
      "Episode 220\tLast reward: 0.96\tAverage reward: 0.65\n",
      "80.41453552246094 [0.0125024  0.03467572 0.9528218 ]\n",
      "Episode 225\tLast reward: 0.92\tAverage reward: 0.67\n",
      "-15.496134757995605 [0.00327337 0.02033669 0.9763899 ]\n",
      "Episode 230\tLast reward: 0.99\tAverage reward: 0.61\n",
      "-10.289859771728516 [0.00254095 0.00902064 0.9884384 ]\n",
      "Episode 235\tLast reward: 0.00\tAverage reward: 0.56\n",
      "25.8268985748291 [0.05242613 0.13779257 0.8097813 ]\n",
      "Episode 240\tLast reward: 1.00\tAverage reward: 0.61\n",
      "47.195072174072266 [0.00753499 0.03879211 0.9536729 ]\n",
      "Episode 245\tLast reward: 0.95\tAverage reward: 0.69\n",
      "36.40278244018555 [0.00249138 0.01886373 0.97864497]\n",
      "Episode 250\tLast reward: 0.00\tAverage reward: 0.70\n",
      "37.449562072753906 [0.00177676 0.01548344 0.9827398 ]\n",
      "Episode 255\tLast reward: 0.93\tAverage reward: 0.76\n",
      "8.730396270751953 [0.00186156 0.01636009 0.9817784 ]\n",
      "Episode 260\tLast reward: 0.97\tAverage reward: 0.72\n",
      "59.93505096435547 [0.00656844 0.03882615 0.95460546]\n",
      "Episode 265\tLast reward: 0.93\tAverage reward: 0.72\n",
      "37.65837860107422 [0.00154403 0.01562    0.98283607]\n",
      "Episode 270\tLast reward: 0.00\tAverage reward: 0.64\n",
      "42.70159912109375 [0.00568133 0.01975267 0.974566  ]\n",
      "Episode 275\tLast reward: 0.98\tAverage reward: 0.72\n",
      "42.74358367919922 [0.00395899 0.03167316 0.96436775]\n",
      "Episode 280\tLast reward: 0.93\tAverage reward: 0.73\n",
      "40.909305572509766 [0.00378541 0.03201365 0.964201  ]\n",
      "Episode 285\tLast reward: 1.00\tAverage reward: 0.74\n",
      "-17.886730194091797 [8.4807345e-04 1.2644437e-02 9.8650753e-01]\n",
      "Episode 290\tLast reward: 0.00\tAverage reward: 0.70\n",
      "25.637481689453125 [0.00637571 0.04611186 0.94751245]\n",
      "Episode 295\tLast reward: 0.99\tAverage reward: 0.72\n",
      "53.59077453613281 [0.00637991 0.04771385 0.9459063 ]\n",
      "Episode 300\tLast reward: 1.00\tAverage reward: 0.69\n",
      "24.244159698486328 [0.00241996 0.02776173 0.9698183 ]\n",
      "Episode 305\tLast reward: 0.99\tAverage reward: 0.71\n",
      "27.930496215820312 [0.06486198 0.06386386 0.8712741 ]\n",
      "Episode 310\tLast reward: 0.97\tAverage reward: 0.77\n",
      "32.60457992553711 [0.00289493 0.03422488 0.96288013]\n",
      "Episode 315\tLast reward: 0.00\tAverage reward: 0.77\n",
      "23.742355346679688 [0.00361267 0.04025474 0.9561326 ]\n",
      "Episode 320\tLast reward: 0.93\tAverage reward: 0.69\n",
      "36.79444122314453 [0.2522726  0.305573   0.44215444]\n",
      "Episode 325\tLast reward: 0.00\tAverage reward: 0.66\n",
      "50.29446792602539 [0.0267018  0.13039604 0.8429022 ]\n",
      "Episode 330\tLast reward: 0.93\tAverage reward: 0.69\n",
      "36.805660247802734 [0.02068941 0.11433662 0.864974  ]\n",
      "Episode 335\tLast reward: 0.94\tAverage reward: 0.66\n",
      "-29.215238571166992 [0.0035221  0.04270714 0.9537707 ]\n",
      "Episode 340\tLast reward: 0.00\tAverage reward: 0.56\n",
      "-20.04657745361328 [0.35522953 0.36593565 0.27883488]\n",
      "Episode 345\tLast reward: 0.00\tAverage reward: 0.56\n",
      "5.6924543380737305 [0.00968392 0.07492953 0.91538644]\n",
      "Episode 350\tLast reward: 0.00\tAverage reward: 0.57\n",
      "2.270390510559082 [0.03053869 0.13964273 0.8298185 ]\n",
      "Episode 355\tLast reward: 0.98\tAverage reward: 0.62\n",
      "82.9581069946289 [0.01819776 0.10304322 0.8787591 ]\n",
      "Episode 360\tLast reward: 0.93\tAverage reward: 0.66\n",
      "155.57232666015625 [0.0392384  0.15360178 0.8071598 ]\n",
      "Episode 365\tLast reward: 0.97\tAverage reward: 0.72\n",
      "33.84484100341797 [0.01812628 0.09685084 0.8850229 ]\n",
      "Episode 370\tLast reward: 0.97\tAverage reward: 0.78\n",
      "67.71451568603516 [0.00477338 0.02275904 0.97246754]\n",
      "Episode 375\tLast reward: 0.00\tAverage reward: 0.77\n",
      "98.97137451171875 [0.06935566 0.19418629 0.73645806]\n",
      "Episode 380\tLast reward: 0.93\tAverage reward: 0.77\n",
      "19.24694061279297 [0.00826353 0.03321307 0.95852345]\n",
      "Episode 385\tLast reward: 0.00\tAverage reward: 0.72\n",
      "63.80286407470703 [0.00431768 0.03460005 0.96108234]\n",
      "Episode 390\tLast reward: 0.99\tAverage reward: 0.73\n",
      "36.9094123840332 [0.0049035  0.03586918 0.9592273 ]\n",
      "Episode 395\tLast reward: 0.95\tAverage reward: 0.78\n",
      "16.372615814208984 [0.00803996 0.04744305 0.944517  ]\n",
      "Episode 400\tLast reward: 0.95\tAverage reward: 0.82\n",
      "15.093832015991211 [0.00912838 0.04982889 0.9410427 ]\n",
      "Episode 405\tLast reward: 0.97\tAverage reward: 0.82\n",
      "44.08302307128906 [0.00202484 0.01794681 0.9800283 ]\n",
      "Episode 410\tLast reward: 0.94\tAverage reward: 0.76\n",
      "46.64295196533203 [8.1712475e-05 1.9598983e-03 9.9795842e-01]\n",
      "Episode 415\tLast reward: 0.00\tAverage reward: 0.75\n",
      "24.371723175048828 [7.6349324e-04 8.3647594e-03 9.9087173e-01]\n",
      "Episode 420\tLast reward: 0.96\tAverage reward: 0.76\n",
      "36.20243453979492 [0.00184918 0.01465016 0.9835007 ]\n",
      "Episode 425\tLast reward: 0.93\tAverage reward: 0.76\n",
      "13.10076904296875 [0.01579541 0.06171983 0.92248476]\n",
      "Episode 430\tLast reward: 0.97\tAverage reward: 0.76\n",
      "70.6659164428711 [0.00424021 0.02470389 0.97105587]\n",
      "Episode 435\tLast reward: 0.94\tAverage reward: 0.76\n",
      "37.4117546081543 [0.00485226 0.02683859 0.9683091 ]\n",
      "Episode 440\tLast reward: 0.98\tAverage reward: 0.77\n",
      "43.40528869628906 [0.00589392 0.03087796 0.9632282 ]\n",
      "Episode 445\tLast reward: 0.97\tAverage reward: 0.81\n",
      "34.921607971191406 [0.02362688 0.0799609  0.89641225]\n",
      "Episode 450\tLast reward: 0.96\tAverage reward: 0.84\n",
      "66.0814437866211 [0.00323244 0.02064318 0.9761244 ]\n",
      "Episode 455\tLast reward: 0.00\tAverage reward: 0.82\n",
      "58.326690673828125 [0.00910291 0.04219359 0.94870347]\n",
      "Episode 460\tLast reward: 0.95\tAverage reward: 0.85\n",
      "32.16251754760742 [9.7188133e-01 4.8753133e-04 2.7631186e-02]\n",
      "Episode 465\tLast reward: 0.00\tAverage reward: 0.78\n",
      "34.336517333984375 [0.00376242 0.02312344 0.97311413]\n",
      "Episode 470\tLast reward: 0.95\tAverage reward: 0.82\n",
      "38.21335220336914 [0.00785866 0.03853262 0.95360875]\n",
      "Episode 475\tLast reward: 0.93\tAverage reward: 0.82\n",
      "19.10977554321289 [0.00411846 0.02473375 0.9711477 ]\n",
      "Episode 480\tLast reward: 0.99\tAverage reward: 0.80\n",
      "-8.138747215270996 [0.17151183 0.23421556 0.5942726 ]\n",
      "Episode 485\tLast reward: 0.98\tAverage reward: 0.80\n",
      "69.93196105957031 [0.00336498 0.02178015 0.9748548 ]\n",
      "Episode 490\tLast reward: 0.94\tAverage reward: 0.83\n",
      "-20.79973602294922 [0.14904384 0.19560109 0.65535504]\n",
      "Episode 495\tLast reward: 0.00\tAverage reward: 0.77\n",
      "22.927902221679688 [0.00196025 0.01536566 0.98267406]\n",
      "Episode 500\tLast reward: 0.00\tAverage reward: 0.77\n",
      "18.874317169189453 [0.02029566 0.04828524 0.9314191 ]\n",
      "Episode 505\tLast reward: 0.95\tAverage reward: 0.77\n",
      "26.574344635009766 [0.07350076 0.16686106 0.7596382 ]\n",
      "Episode 510\tLast reward: 0.00\tAverage reward: 0.72\n",
      "34.80405044555664 [0.00236275 0.01834475 0.9792925 ]\n",
      "Episode 515\tLast reward: 0.97\tAverage reward: 0.73\n",
      "30.96499252319336 [0.10008883 0.00943268 0.89047855]\n",
      "Episode 520\tLast reward: 0.98\tAverage reward: 0.78\n",
      "36.66217803955078 [0.00713148 0.03965518 0.95321345]\n",
      "Episode 525\tLast reward: 0.94\tAverage reward: 0.78\n",
      "35.01847839355469 [0.00139557 0.01337109 0.98523337]\n",
      "Episode 530\tLast reward: 0.97\tAverage reward: 0.78\n",
      "53.502464294433594 [0.00793821 0.04306557 0.9489963 ]\n",
      "Episode 535\tLast reward: 0.98\tAverage reward: 0.78\n",
      "20.078344345092773 [0.00282587 0.02179902 0.97537506]\n",
      "Episode 540\tLast reward: 0.99\tAverage reward: 0.78\n",
      "22.820907592773438 [0.01398285 0.06307457 0.9229425 ]\n",
      "Episode 545\tLast reward: 0.98\tAverage reward: 0.82\n",
      "28.164758682250977 [0.00177454 0.01626853 0.98195696]\n",
      "Episode 550\tLast reward: 0.94\tAverage reward: 0.81\n",
      "10.362988471984863 [0.01105663 0.05538012 0.93356335]\n",
      "Episode 555\tLast reward: 0.96\tAverage reward: 0.85\n",
      "7.196723937988281 [0.00686362 0.0412213  0.95191497]\n",
      "Episode 560\tLast reward: 0.97\tAverage reward: 0.88\n",
      "48.608558654785156 [0.01072301 0.05061458 0.9386624 ]\n",
      "Episode 565\tLast reward: 0.93\tAverage reward: 0.90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32m/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb Cell 6\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ep_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m):  \u001b[39m# Don't infinite loop while learning\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     action,show_prob \u001b[39m=\u001b[39m select_action(state)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m#if args.render:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m#    env.render()\u001b[39;00m\n",
      "\u001b[1;32m/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb Cell 6\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_action\u001b[39m(state):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(state)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     probs,state_value \u001b[39m=\u001b[39m policy(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     m \u001b[39m=\u001b[39m Categorical(probs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     action \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39msample()\n",
      "File \u001b[0;32m~/mambaforge/envs/deep_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb Cell 6\u001b[0m in \u001b[0;36mPolicy.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutconvsize)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maffine1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# h = self.rnn(x.unsqueeze(0), self.hidden_state)[1]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# self.hidden_state = h\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# out = h.squeeze(0)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m action_prob \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_head(out), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/deep_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/deep_env/lib/python3.10/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/mambaforge/envs/deep_env/lib/python3.10/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(policy.state_dict(), 'miniworld_task.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "policy.load_state_dict(torch.load('../saved_models/miniworld_task.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections limits [-1, 6, 11, 20]\n",
      "sections lengths [7, 5, 9]\n",
      "sections motor gains [1, 1, 1]\n",
      "tensor([[0.1310, 0.2161, 0.6528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2200, 0.2478, 0.5321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2360, 0.2537, 0.5103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1132, 0.1756, 0.7112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3268, 0.3173, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1132, 0.1756, 0.7112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1845, 0.2617, 0.5539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2162, 0.2446, 0.5392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1005, 0.1476, 0.7519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2648, 0.2825, 0.4527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5036, 0.2064, 0.2901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2054, 0.2526, 0.5420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5036, 0.2064, 0.2901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2054, 0.2526, 0.5420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2102, 0.1900, 0.5998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2862, 0.3126, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2022, 0.2558, 0.5420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2862, 0.3126, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2022, 0.2558, 0.5420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2862, 0.3126, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2102, 0.1900, 0.5998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2862, 0.3126, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1666, 0.2141, 0.6193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2017, 0.2181, 0.5802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2017, 0.2181, 0.5802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1726, 0.2062, 0.6212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2190, 0.2280, 0.5530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1097, 0.1558, 0.7345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1773, 0.1769, 0.6458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0708, 0.0952, 0.8340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2883, 0.2887, 0.4230]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2127, 0.2579, 0.5294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1917, 0.2458, 0.5624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2390, 0.2530, 0.5079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1917, 0.2458, 0.5624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2390, 0.2530, 0.5079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1836, 0.1856, 0.6308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3236, 0.2986, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1851, 0.1999, 0.6150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2035, 0.2210, 0.5755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2454, 0.2841, 0.4705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3448, 0.3204, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3448, 0.3204, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3267, 0.3297, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1747, 0.2269, 0.5984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1806, 0.2115, 0.6080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2319, 0.2593, 0.5088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2319, 0.2593, 0.5088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2999, 0.2919, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2999, 0.2919, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2999, 0.2919, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2353, 0.2756, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2999, 0.2919, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2229, 0.2648, 0.5123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3086, 0.3244, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3086, 0.3244, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2393, 0.3200, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2393, 0.3200, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2393, 0.3200, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2890, 0.3150, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2890, 0.3150, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2890, 0.3150, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2593, 0.3188, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4224, 0.3441, 0.2336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2890, 0.3150, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1600, 0.2451, 0.5949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1600, 0.2451, 0.5949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1600, 0.2451, 0.5949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2628, 0.2883, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2169, 0.2513, 0.5318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2024, 0.2429, 0.5546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1212, 0.1800, 0.6989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2076, 0.2592, 0.5332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2342, 0.2741, 0.4918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3155, 0.3559, 0.3286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2342, 0.2741, 0.4918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2342, 0.2741, 0.4918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2925, 0.3235, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2925, 0.3235, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2925, 0.3235, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2233, 0.2342, 0.5425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2021, 0.2686, 0.5294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2021, 0.2686, 0.5294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2450, 0.3179, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1649, 0.2432, 0.5919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0722, 0.1072, 0.8206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0328, 0.0446, 0.9226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0243, 0.0501, 0.9256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0208, 0.0508, 0.9285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0263, 0.0400, 0.9337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0221, 0.0413, 0.9365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0224, 0.0386, 0.9390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0220, 0.0475, 0.9305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0204, 0.0391, 0.9405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0328, 0.0567, 0.9105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0164, 0.0348, 0.9488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0120, 0.0346, 0.9535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0236, 0.0447, 0.9316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0200, 0.0360, 0.9440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0134, 0.0367, 0.9500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0165, 0.0382, 0.9452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0118, 0.0239, 0.9642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0146, 0.0322, 0.9533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0155, 0.0383, 0.9462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0098, 0.0223, 0.9679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0161, 0.0348, 0.9491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0168, 0.0327, 0.9505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0065, 0.0173, 0.9762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0101, 0.0276, 0.9624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0107, 0.0211, 0.9682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0059, 0.0145, 0.9796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0090, 0.0253, 0.9657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0474, 0.0978, 0.8548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0090, 0.0253, 0.9657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0076, 0.0160, 0.9764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0074, 0.0183, 0.9743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0105, 0.0219, 0.9676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0055, 0.0150, 0.9794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0157, 0.0353, 0.9490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0093, 0.0205, 0.9702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0565, 0.1492, 0.7943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0093, 0.0205, 0.9702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0066, 0.0168, 0.9766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0134, 0.0389, 0.9477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0086, 0.0176, 0.9737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0060, 0.0191, 0.9749]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0137, 0.0344, 0.9519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0050, 0.0112, 0.9838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0107, 0.0263, 0.9630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0086, 0.0257, 0.9657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0066, 0.0150, 0.9784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0108, 0.0297, 0.9595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0094, 0.0215, 0.9691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0060, 0.0167, 0.9773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0173, 0.0508, 0.9319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0061, 0.0146, 0.9792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0060, 0.0141, 0.9799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0081, 0.0254, 0.9665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0068, 0.0197, 0.9736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0097, 0.0207, 0.9696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0057, 0.0152, 0.9791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0091, 0.0267, 0.9642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0157, 0.0409, 0.9434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0046, 0.0135, 0.9819]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0063, 0.0149, 0.9787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0079, 0.0256, 0.9666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0038, 0.0112, 0.9849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0038, 0.0100, 0.9862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0039, 0.0091, 0.9871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0018, 0.0079, 0.9903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0046, 0.0140, 0.9814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0030, 0.0080, 0.9890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0026, 0.0072, 0.9902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0099, 0.0340, 0.9561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0028, 0.0090, 0.9882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0037, 0.0103, 0.9860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0044, 0.0119, 0.9837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0018, 0.0076, 0.9906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.0046, 0.9944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0033, 0.0096, 0.9871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0019, 0.0064, 0.9917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0013, 0.0063, 0.9924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.7550e-04, 4.6249e-03, 9.9440e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0016, 0.0065, 0.9920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.1792e-04, 4.1061e-03, 9.9508e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.7805e-04, 3.2616e-03, 9.9616e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.0057, 0.9932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.4938e-04, 3.2787e-03, 9.9617e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.7925e-04, 2.4816e-03, 9.9714e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.9425e-04, 5.3915e-03, 9.9371e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[7.3613e-04, 4.6480e-03, 9.9462e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.4353e-04, 5.0436e-03, 9.9411e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0015, 0.0086, 0.9899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.7635e-04, 6.2188e-03, 9.9280e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0012, 0.0078, 0.9911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "test_args = dict(\n",
    "    nb_sections=3,\n",
    "    proba_change_motor_gain=0,\n",
    "    min_section_length=5,\n",
    "    max_section_length=10,\n",
    "    training=False,\n",
    "    max_episode_steps=250,\n",
    ")\n",
    "env = gym.make('MiniWorld-TaskHallway-v0', view=\"agent\", render_mode=\"human\",**test_args)\n",
    "# env = gym.make('MiniWorld-Hallway-v0', length = 15, view=\"agent\", render_mode=\"human\")\n",
    "env = PyTorchObsWrapper(env)\n",
    "\n",
    "import timeit\n",
    "class CodeTimer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (timeit.default_timer() - self.start) * 1000.0\n",
    "        print('Code block' + self.name + ' took: ' + str(self.took) + ' ms')\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Create the display window\n",
    "env.render()\n",
    "\n",
    "for _ in range(test_args['max_episode_steps']):\n",
    "    action,probs = select_action(observation) # agent policy that uses the observation and info\n",
    "    print(probs)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        print('over')\n",
    "        observation, info = env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m policy\u001b[39m.\u001b[39mvalue_head[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(policy\u001b[39m.\u001b[39mvalue_head[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felix/Documents/phd/ibl/serotonin/src/miniworld/old/main.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "policy.value_head[0].weight\n",
    "plt.imshow(policy.value_head[0].weight.cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy.action_head[0].weight\n",
    "plt.imshow(policy.action_head[0].weight.cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c269a8ac1ce32886f1d361a7d7752f19c27b7d8ebd8d0fb0e1cb07b13dddb563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
