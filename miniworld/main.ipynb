{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import pyglet\n",
    "from pyglet.window import key\n",
    "\n",
    "import miniworld\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "from miniworld.wrappers import PyTorchObsWrapper,GreyscaleWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MiniWorld-CollectHealth-v0',\n",
       " 'MiniWorld-FourRooms-v0',\n",
       " 'MiniWorld-Hallway-v0',\n",
       " 'MiniWorld-Maze-v0',\n",
       " 'MiniWorld-MazeS2-v0',\n",
       " 'MiniWorld-MazeS3-v0',\n",
       " 'MiniWorld-MazeS3Fast-v0',\n",
       " 'MiniWorld-OneRoom-v0',\n",
       " 'MiniWorld-OneRoomS6-v0',\n",
       " 'MiniWorld-OneRoomS6Fast-v0',\n",
       " 'MiniWorld-PickupObjects-v0',\n",
       " 'MiniWorld-PutNext-v0',\n",
       " 'MiniWorld-RoomObjects-v0',\n",
       " 'MiniWorld-Sidewalk-v0',\n",
       " 'MiniWorld-Sign-v0',\n",
       " 'MiniWorld-TMaze-v0',\n",
       " 'MiniWorld-TMazeLeft-v0',\n",
       " 'MiniWorld-TMazeRight-v0',\n",
       " 'MiniWorld-ThreeRooms-v0',\n",
       " 'MiniWorld-WallGap-v0',\n",
       " 'MiniWorld-YMaze-v0',\n",
       " 'MiniWorld-YMazeLeft-v0',\n",
       " 'MiniWorld-YMazeRight-v0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miniworld.envs.env_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MiniWorld-OneRoom-v0', view=\"agent\", render_mode=None)\n",
    "env = PyTorchObsWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.hidden_size = 32\n",
    "        self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2).to(device)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2).to(device)\n",
    "        self.maxpool = nn.MaxPool2d(2).to(device)\n",
    "\n",
    "        self.outconvsize = 2016\n",
    "        self.affine1 = nn.Linear( self.outconvsize , self.hidden_size).to(device) \n",
    "        # self.rnn = nn.RNN(self.hidden_size, self.hidden_size).to(device)\n",
    "\n",
    "        \n",
    "        self.action_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 3).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_size, self.hidden_size).to(device),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(self.hidden_size, 1).to(device)\n",
    "                                ).to(device)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.batch_loss = []\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.any(torch.isnan(x)):\n",
    "            print(\"NAN in input\")\n",
    "        x = self.relu(self.conv1(x))\n",
    "        if torch.any(torch.isnan(x)):\n",
    "            print(\"NAN in issnput\")\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(-1,self.outconvsize)\n",
    "        x = self.relu(self.affine1(x))\n",
    "        #h = self.rnn(x.unsqueeze(0), self.hidden_state)[1]\n",
    "        #self.hidden_state = h\n",
    "        h = x\n",
    "        action_prob = F.softmax(self.action_head(h), dim=-1)\n",
    "\n",
    "        state_value = self.value_head(h)\n",
    "\n",
    "        return action_prob, state_value\n",
    "\n",
    "\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs,state_value = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "\n",
    "    policy.saved_log_probs.append( (m.log_prob(action), state_value) )\n",
    "\n",
    "    return action.item(),probs\n",
    "    \n",
    "gamma = 1\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + eps) # this create error ...\n",
    "\n",
    "    for (log_prob,value), R in zip(policy.saved_log_probs, returns):\n",
    "        advantage = R - value.item()\n",
    "        policy_loss.append(-log_prob * advantage)\n",
    "        value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n",
    "    loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "    policy.batch_loss.append(loss)\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    policy.reset_hidden_state()\n",
    "\n",
    "\n",
    "def finish_batch():\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy.batch_loss).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.batch_loss[:]\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71876"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in range(1000):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 100):  # Don't infinite loop while learning\n",
    "            action,show_prob = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            #if args.render:\n",
    "            #    env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 5 == 0:\n",
    "            loss = finish_batch()\n",
    "            print(loss,show_prob.cpu().detach().numpy()[0])\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fphub\\AppData\\Local\\Temp\\ipykernel_1980\\3013081349.py:90: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss.append(F.smooth_l1_loss(value, torch.tensor([R]).to(device)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.733391761779785 [0.3303635  0.34034482 0.32929167]\n",
      "Episode 0\tLast reward: 0.00\tAverage reward: 0.00\n",
      "45.933387756347656 [0.33055696 0.3401594  0.32928365]\n",
      "Episode 5\tLast reward: 0.00\tAverage reward: 0.04\n",
      "-24.261735916137695 [0.3307736  0.34005666 0.32916972]\n",
      "Episode 10\tLast reward: 0.00\tAverage reward: 0.03\n",
      "125.9843521118164 [0.30684444 0.3918668  0.30128875]\n",
      "Episode 15\tLast reward: 0.93\tAverage reward: 0.15\n",
      "43.91679763793945 [0.21132502 0.2564801  0.53219485]\n",
      "Episode 20\tLast reward: 0.92\tAverage reward: 0.21\n",
      "30.391714096069336 [0.30844256 0.32374692 0.3678106 ]\n",
      "Episode 25\tLast reward: 0.00\tAverage reward: 0.20\n",
      "-46.39183807373047 [0.31099382 0.3416136  0.3473926 ]\n",
      "Episode 30\tLast reward: 0.00\tAverage reward: 0.15\n",
      "-6.866427898406982 [0.07187061 0.07359064 0.85453874]\n",
      "Episode 35\tLast reward: 0.00\tAverage reward: 0.20\n",
      "141.3742218017578 [0.18315165 0.17984113 0.63700724]\n",
      "Episode 40\tLast reward: 0.91\tAverage reward: 0.28\n",
      "-2.453948497772217 [0.3308327  0.3398943  0.32927305]\n",
      "Episode 45\tLast reward: 0.00\tAverage reward: 0.30\n",
      "66.12223815917969 [0.3155393  0.33578998 0.3486707 ]\n",
      "Episode 50\tLast reward: 0.00\tAverage reward: 0.36\n",
      "-36.38426208496094 [0.29862964 0.32921913 0.3721512 ]\n",
      "Episode 55\tLast reward: 0.00\tAverage reward: 0.28\n",
      "-46.81752014160156 [0.3295427  0.34011334 0.33034396]\n",
      "Episode 60\tLast reward: 0.00\tAverage reward: 0.21\n",
      "118.20811462402344 [0.31648728 0.3373971  0.34611562]\n",
      "Episode 65\tLast reward: 0.00\tAverage reward: 0.24\n",
      "59.80608367919922 [0.32859942 0.3407746  0.33062595]\n",
      "Episode 70\tLast reward: 0.00\tAverage reward: 0.27\n",
      "-20.092050552368164 [0.327983   0.34071663 0.33130035]\n",
      "Episode 75\tLast reward: 0.00\tAverage reward: 0.25\n",
      "-31.592432022094727 [0.11009348 0.11668475 0.7732217 ]\n",
      "Episode 80\tLast reward: 0.00\tAverage reward: 0.20\n",
      "-35.151214599609375 [0.28304648 0.33895263 0.37800083]\n",
      "Episode 85\tLast reward: 0.00\tAverage reward: 0.15\n",
      "70.36582946777344 [0.31493044 0.3504402  0.33462933]\n",
      "Episode 90\tLast reward: 0.00\tAverage reward: 0.16\n",
      "109.44947814941406 [0.12070605 0.13421789 0.745076  ]\n",
      "Episode 95\tLast reward: 0.96\tAverage reward: 0.25\n",
      "-39.254417419433594 [0.32426688 0.3425641  0.33316898]\n",
      "Episode 100\tLast reward: 0.00\tAverage reward: 0.19\n",
      "-35.878074645996094 [0.324752  0.3423294 0.3329186]\n",
      "Episode 105\tLast reward: 0.00\tAverage reward: 0.15\n",
      "-36.162620544433594 [0.32414258 0.34285435 0.33300304]\n",
      "Episode 110\tLast reward: 0.00\tAverage reward: 0.12\n",
      "-28.513750076293945 [0.3239469  0.34283206 0.33322102]\n",
      "Episode 115\tLast reward: 0.00\tAverage reward: 0.09\n",
      "-36.823123931884766 [0.32173526 0.3436744  0.33459038]\n",
      "Episode 120\tLast reward: 0.00\tAverage reward: 0.07\n",
      "7.147014617919922 [0.29721582 0.3080786  0.3947056 ]\n",
      "Episode 125\tLast reward: 0.00\tAverage reward: 0.13\n",
      "-17.80449676513672 [0.21250205 0.30369318 0.48380473]\n",
      "Episode 130\tLast reward: 0.00\tAverage reward: 0.15\n",
      "50.423377990722656 [0.2659935  0.3040011  0.43000543]\n",
      "Episode 135\tLast reward: 0.94\tAverage reward: 0.21\n",
      "9.412969589233398 [0.08473584 0.17280674 0.7424574 ]\n",
      "Episode 140\tLast reward: 0.96\tAverage reward: 0.21\n",
      "38.64991760253906 [0.30984205 0.32946634 0.36069167]\n",
      "Episode 145\tLast reward: 0.00\tAverage reward: 0.21\n",
      "6.25926399230957 [0.09413784 0.08090733 0.8249548 ]\n",
      "Episode 150\tLast reward: 0.94\tAverage reward: 0.21\n",
      "56.861446380615234 [0.3158124  0.33916333 0.34502432]\n",
      "Episode 155\tLast reward: 0.00\tAverage reward: 0.24\n",
      "11.079927444458008 [0.05935054 0.07196522 0.8686843 ]\n",
      "Episode 160\tLast reward: 0.95\tAverage reward: 0.28\n",
      "-28.224857330322266 [0.26841807 0.30708063 0.42450133]\n",
      "Episode 165\tLast reward: 0.00\tAverage reward: 0.22\n",
      "45.18278884887695 [0.0768562  0.36940393 0.55373985]\n",
      "Episode 170\tLast reward: 0.96\tAverage reward: 0.25\n",
      "48.327369689941406 [0.0227151  0.04419712 0.9330878 ]\n",
      "Episode 175\tLast reward: 0.93\tAverage reward: 0.37\n",
      "-27.565006256103516 [0.24392302 0.28508455 0.4709925 ]\n",
      "Episode 180\tLast reward: 0.00\tAverage reward: 0.33\n",
      "44.67261505126953 [0.17609218 0.24831504 0.5755928 ]\n",
      "Episode 185\tLast reward: 0.00\tAverage reward: 0.34\n",
      "-23.02178192138672 [0.1676982  0.23390862 0.5983932 ]\n",
      "Episode 190\tLast reward: 0.00\tAverage reward: 0.30\n",
      "19.723318099975586 [0.12338623 0.11955179 0.757062  ]\n",
      "Episode 195\tLast reward: 0.95\tAverage reward: 0.37\n",
      "-8.298188209533691 [0.0072719  0.08212879 0.91059935]\n",
      "Episode 200\tLast reward: 0.00\tAverage reward: 0.33\n",
      "-23.893470764160156 [0.25393477 0.2997876  0.44627756]\n",
      "Episode 205\tLast reward: 0.00\tAverage reward: 0.29\n",
      "-27.165660858154297 [0.00670476 0.00446013 0.98883504]\n",
      "Episode 210\tLast reward: 0.00\tAverage reward: 0.23\n",
      "-35.61170959472656 [0.29864898 0.33207232 0.36927873]\n",
      "Episode 215\tLast reward: 0.00\tAverage reward: 0.18\n",
      "54.625797271728516 [0.04202626 0.04724335 0.91073036]\n",
      "Episode 220\tLast reward: 0.94\tAverage reward: 0.23\n",
      "40.84242248535156 [0.20393589 0.29189038 0.5041737 ]\n",
      "Episode 225\tLast reward: 0.00\tAverage reward: 0.31\n",
      "42.640167236328125 [0.31775913 0.3456584  0.33658254]\n",
      "Episode 230\tLast reward: 0.00\tAverage reward: 0.28\n",
      "107.38127136230469 [0.3176953  0.34615737 0.33614737]\n",
      "Episode 235\tLast reward: 0.00\tAverage reward: 0.34\n",
      "-21.11211585998535 [0.23649426 0.3502896  0.41321617]\n",
      "Episode 240\tLast reward: 0.00\tAverage reward: 0.26\n",
      "100.36683654785156 [0.26714405 0.3126749  0.420181  ]\n",
      "Episode 245\tLast reward: 0.00\tAverage reward: 0.29\n",
      "-24.82068634033203 [0.31671447 0.34781796 0.33546755]\n",
      "Episode 250\tLast reward: 0.00\tAverage reward: 0.22\n",
      "66.906005859375 [0.24947569 0.3064114  0.4441129 ]\n",
      "Episode 255\tLast reward: 0.95\tAverage reward: 0.30\n",
      "108.99040985107422 [0.25916436 0.31233135 0.42850432]\n",
      "Episode 260\tLast reward: 0.96\tAverage reward: 0.36\n",
      "103.17019653320312 [0.30553105 0.35000467 0.34446427]\n",
      "Episode 265\tLast reward: 0.00\tAverage reward: 0.36\n",
      "23.04784393310547 [0.3151139  0.34885696 0.33602917]\n",
      "Episode 270\tLast reward: 0.00\tAverage reward: 0.37\n",
      "25.443519592285156 [0.3148488  0.34903243 0.33611873]\n",
      "Episode 275\tLast reward: 0.00\tAverage reward: 0.32\n",
      "-24.736953735351562 [0.3127601  0.34943146 0.3378084 ]\n",
      "Episode 280\tLast reward: 0.00\tAverage reward: 0.29\n",
      "-28.9556884765625 [0.11318263 0.56689584 0.31992152]\n",
      "Episode 285\tLast reward: 0.00\tAverage reward: 0.22\n",
      "122.99689483642578 [0.03483955 0.04375126 0.9214092 ]\n",
      "Episode 290\tLast reward: 0.94\tAverage reward: 0.26\n",
      "2.7014694213867188 [0.02042314 0.02500957 0.95456725]\n",
      "Episode 295\tLast reward: 0.00\tAverage reward: 0.24\n",
      "63.95469665527344 [0.06904508 0.08642191 0.8445329 ]\n",
      "Episode 300\tLast reward: 0.90\tAverage reward: 0.27\n",
      "-40.48693084716797 [0.31068403 0.3596803  0.32963562]\n",
      "Episode 305\tLast reward: 0.00\tAverage reward: 0.21\n",
      "49.49432373046875 [0.01942269 0.05704495 0.92353237]\n",
      "Episode 310\tLast reward: 0.00\tAverage reward: 0.29\n",
      "41.34401321411133 [0.08333457 0.09143912 0.8252263 ]\n",
      "Episode 315\tLast reward: 0.91\tAverage reward: 0.36\n",
      "3.0860252380371094 [0.29941142 0.33734497 0.3632436 ]\n",
      "Episode 320\tLast reward: 0.00\tAverage reward: 0.37\n",
      "69.94654846191406 [0.31439137 0.35061142 0.33499718]\n",
      "Episode 325\tLast reward: 0.00\tAverage reward: 0.41\n",
      "55.85565185546875 [0.08987568 0.10849406 0.80163026]\n",
      "Episode 330\tLast reward: 0.98\tAverage reward: 0.45\n",
      "67.12771606445312 [0.3140978  0.35049435 0.33540785]\n",
      "Episode 335\tLast reward: 0.00\tAverage reward: 0.47\n",
      "77.28059387207031 [0.09113727 0.10464771 0.80421495]\n",
      "Episode 340\tLast reward: 0.92\tAverage reward: 0.49\n",
      "82.80201721191406 [0.21149923 0.2846423  0.50385845]\n",
      "Episode 345\tLast reward: 0.00\tAverage reward: 0.50\n",
      "16.3736515045166 [0.31502685 0.35166186 0.33331126]\n",
      "Episode 350\tLast reward: 0.00\tAverage reward: 0.52\n",
      "13.648902893066406 [0.30927745 0.34375072 0.34697184]\n",
      "Episode 355\tLast reward: 0.00\tAverage reward: 0.44\n",
      "-20.063716888427734 [0.23380989 0.39335558 0.37283453]\n",
      "Episode 360\tLast reward: 0.00\tAverage reward: 0.38\n",
      "89.93731689453125 [0.30760393 0.38337064 0.3090255 ]\n",
      "Episode 365\tLast reward: 0.00\tAverage reward: 0.42\n",
      "-56.40644454956055 [0.15590495 0.5933274  0.2507676 ]\n",
      "Episode 370\tLast reward: 0.00\tAverage reward: 0.37\n",
      "-38.28228759765625 [0.314849   0.35453913 0.33061185]\n",
      "Episode 375\tLast reward: 0.00\tAverage reward: 0.33\n",
      "-18.352685928344727 [0.31531566 0.35502875 0.3296556 ]\n",
      "Episode 380\tLast reward: 0.00\tAverage reward: 0.34\n",
      "138.87362670898438 [0.25598183 0.3529225  0.3910956 ]\n",
      "Episode 385\tLast reward: 0.00\tAverage reward: 0.43\n",
      "-47.78804016113281 [0.24343248 0.38888398 0.3676835 ]\n",
      "Episode 390\tLast reward: 0.00\tAverage reward: 0.33\n",
      "-13.65664291381836 [0.18483207 0.25712222 0.5580456 ]\n",
      "Episode 395\tLast reward: 0.00\tAverage reward: 0.30\n",
      "10.869733810424805 [0.3154889  0.35661206 0.32789898]\n",
      "Episode 400\tLast reward: 0.00\tAverage reward: 0.27\n",
      "-23.221389770507812 [0.22152656 0.27278298 0.50569046]\n",
      "Episode 405\tLast reward: 0.00\tAverage reward: 0.26\n",
      "-6.028919219970703 [0.3160189  0.3564744  0.32750672]\n",
      "Episode 410\tLast reward: 0.00\tAverage reward: 0.28\n",
      "-5.731218338012695 [0.31500837 0.35717732 0.32781425]\n",
      "Episode 415\tLast reward: 0.00\tAverage reward: 0.26\n",
      "30.19194793701172 [0.00234433 0.00199675 0.995659  ]\n",
      "Episode 420\tLast reward: 0.00\tAverage reward: 0.24\n",
      "22.813798904418945 [0.19358474 0.19952361 0.60689163]\n",
      "Episode 425\tLast reward: 0.92\tAverage reward: 0.27\n",
      "78.06706237792969 [0.23668243 0.35882762 0.4044899 ]\n",
      "Episode 430\tLast reward: 0.00\tAverage reward: 0.34\n",
      "-49.81969451904297 [0.27587357 0.31300145 0.41112497]\n",
      "Episode 435\tLast reward: 0.00\tAverage reward: 0.26\n",
      "52.90156555175781 [0.26796192 0.4266663  0.3053718 ]\n",
      "Episode 440\tLast reward: 0.00\tAverage reward: 0.33\n",
      "14.949359893798828 [0.09571809 0.08795386 0.81632805]\n",
      "Episode 445\tLast reward: 0.96\tAverage reward: 0.39\n",
      "-9.385780334472656 [0.27549043 0.4283556  0.29615396]\n",
      "Episode 450\tLast reward: 0.00\tAverage reward: 0.43\n",
      "70.66859436035156 [0.1205112  0.20526807 0.67422074]\n",
      "Episode 455\tLast reward: 0.96\tAverage reward: 0.46\n",
      "-12.363163948059082 [0.30060658 0.4299522  0.26944128]\n",
      "Episode 460\tLast reward: 0.94\tAverage reward: 0.44\n",
      "88.97824096679688 [0.04061181 0.13101745 0.82837075]\n",
      "Episode 465\tLast reward: 0.00\tAverage reward: 0.47\n",
      "52.42803192138672 [0.14606105 0.18588364 0.66805536]\n",
      "Episode 470\tLast reward: 0.00\tAverage reward: 0.45\n",
      "-31.509050369262695 [0.3123576  0.3602361  0.32740626]\n",
      "Episode 475\tLast reward: 0.00\tAverage reward: 0.43\n",
      "139.81268310546875 [0.12451293 0.2893777  0.58610934]\n",
      "Episode 480\tLast reward: 0.89\tAverage reward: 0.46\n",
      "86.0406723022461 [0.11750284 0.17988294 0.7026142 ]\n",
      "Episode 485\tLast reward: 0.98\tAverage reward: 0.48\n",
      "88.70508575439453 [0.15333451 0.18999027 0.6566752 ]\n",
      "Episode 490\tLast reward: 0.95\tAverage reward: 0.50\n",
      "61.54267501831055 [0.25742736 0.4016023  0.3409703 ]\n",
      "Episode 495\tLast reward: 0.00\tAverage reward: 0.47\n",
      "6.763195037841797 [0.24273558 0.35990706 0.3973573 ]\n",
      "Episode 500\tLast reward: 0.00\tAverage reward: 0.49\n",
      "-21.474876403808594 [9.0757548e-04 2.1137332e-03 9.9697876e-01]\n",
      "Episode 505\tLast reward: 0.00\tAverage reward: 0.42\n",
      "26.91006088256836 [0.04377824 0.0653321  0.89088976]\n",
      "Episode 510\tLast reward: 0.96\tAverage reward: 0.41\n",
      "57.54447555541992 [0.04321202 0.06221758 0.89457035]\n",
      "Episode 515\tLast reward: 0.95\tAverage reward: 0.45\n",
      "82.1324462890625 [0.06036912 0.10215423 0.8374766 ]\n",
      "Episode 520\tLast reward: 0.96\tAverage reward: 0.57\n",
      "-3.912477493286133 [0.21840648 0.40174472 0.3798488 ]\n",
      "Episode 525\tLast reward: 0.00\tAverage reward: 0.52\n",
      "29.748470306396484 [0.03726598 0.09758196 0.86515206]\n",
      "Episode 530\tLast reward: 0.98\tAverage reward: 0.58\n",
      "143.8656768798828 [0.0367523 0.0811143 0.8821334]\n",
      "Episode 535\tLast reward: 0.94\tAverage reward: 0.62\n",
      "16.47612762451172 [0.03557781 0.06710423 0.897318  ]\n",
      "Episode 540\tLast reward: 0.90\tAverage reward: 0.61\n",
      "54.49497985839844 [0.01822247 0.07670131 0.90507627]\n",
      "Episode 545\tLast reward: 0.95\tAverage reward: 0.60\n",
      "46.02928161621094 [8.6476939e-05 8.7251334e-04 9.9904102e-01]\n",
      "Episode 550\tLast reward: 0.00\tAverage reward: 0.63\n",
      "111.26091766357422 [0.01173241 0.05575634 0.9325113 ]\n",
      "Episode 555\tLast reward: 0.90\tAverage reward: 0.69\n",
      "34.69374084472656 [0.00387256 0.0931061  0.9030214 ]\n",
      "Episode 560\tLast reward: 0.93\tAverage reward: 0.75\n",
      "-6.262661933898926 [0.00129474 0.07532331 0.92338204]\n",
      "Episode 565\tLast reward: 0.94\tAverage reward: 0.79\n",
      "1.954469084739685 [8.3636935e-04 7.1496986e-02 9.2766660e-01]\n",
      "Episode 570\tLast reward: 0.94\tAverage reward: 0.83\n",
      "-20.33255386352539 [0.00250085 0.5770913  0.42040786]\n",
      "Episode 575\tLast reward: 0.92\tAverage reward: 0.86\n",
      "-19.658370971679688 [0.00108249 0.52800405 0.4709135 ]\n",
      "Episode 580\tLast reward: 0.94\tAverage reward: 0.80\n",
      "39.803504943847656 [0.0016295 0.1539905 0.84438  ]\n",
      "Episode 585\tLast reward: 0.92\tAverage reward: 0.83\n",
      "19.844844818115234 [8.3999868e-05 2.4479115e-03 9.9746811e-01]\n",
      "Episode 590\tLast reward: 0.00\tAverage reward: 0.81\n",
      "5.040287017822266 [0.02611483 0.37757847 0.59630674]\n",
      "Episode 595\tLast reward: 1.00\tAverage reward: 0.76\n",
      "86.28752136230469 [0.0140357  0.3711891  0.61477524]\n",
      "Episode 600\tLast reward: 0.00\tAverage reward: 0.71\n",
      "101.30743408203125 [0.01753016 0.21029742 0.7721724 ]\n",
      "Episode 605\tLast reward: 0.94\tAverage reward: 0.72\n",
      "163.43594360351562 [0.01812229 0.14178991 0.84008783]\n",
      "Episode 610\tLast reward: 0.91\tAverage reward: 0.72\n",
      "98.1170425415039 [0.02539588 0.16992767 0.8046765 ]\n",
      "Episode 615\tLast reward: 0.96\tAverage reward: 0.73\n",
      "18.022258758544922 [0.01465167 0.07116272 0.9141857 ]\n",
      "Episode 620\tLast reward: 0.96\tAverage reward: 0.70\n",
      "117.26423645019531 [0.05361517 0.48640475 0.45998013]\n",
      "Episode 625\tLast reward: 0.00\tAverage reward: 0.70\n",
      "20.93279266357422 [0.01909319 0.06744257 0.91346425]\n",
      "Episode 630\tLast reward: 0.93\tAverage reward: 0.67\n",
      "25.805458068847656 [0.02145659 0.08333765 0.8952058 ]\n",
      "Episode 635\tLast reward: 0.93\tAverage reward: 0.69\n",
      "32.475135803222656 [0.01790723 0.16014472 0.82194805]\n",
      "Episode 640\tLast reward: 0.99\tAverage reward: 0.71\n",
      "54.2861442565918 [0.03715674 0.2423574  0.72048587]\n",
      "Episode 645\tLast reward: 0.93\tAverage reward: 0.76\n",
      "6.019597053527832 [0.04483945 0.5955659  0.3595946 ]\n",
      "Episode 650\tLast reward: 0.00\tAverage reward: 0.72\n",
      "28.592248916625977 [0.02158651 0.07311072 0.90530276]\n",
      "Episode 655\tLast reward: 0.96\tAverage reward: 0.72\n",
      "40.42382049560547 [0.02166002 0.11114992 0.86719006]\n",
      "Episode 660\tLast reward: 0.00\tAverage reward: 0.73\n",
      "33.35083770751953 [0.0328683  0.20397507 0.7631566 ]\n",
      "Episode 665\tLast reward: 0.97\tAverage reward: 0.73\n",
      "68.40451049804688 [0.06286568 0.17196245 0.7651718 ]\n",
      "Episode 670\tLast reward: 0.97\tAverage reward: 0.78\n",
      "84.43443298339844 [0.05155097 0.23370506 0.71474403]\n",
      "Episode 675\tLast reward: 0.95\tAverage reward: 0.82\n",
      "58.64796829223633 [0.03528392 0.11623789 0.84847814]\n",
      "Episode 680\tLast reward: 0.96\tAverage reward: 0.85\n",
      "10.433418273925781 [0.03062536 0.13303918 0.8363355 ]\n",
      "Episode 685\tLast reward: 0.97\tAverage reward: 0.83\n",
      "33.224361419677734 [0.02712692 0.09048853 0.8823845 ]\n",
      "Episode 690\tLast reward: 0.89\tAverage reward: 0.86\n",
      "31.46945571899414 [0.01078717 0.07127432 0.9179385 ]\n",
      "Episode 695\tLast reward: 0.96\tAverage reward: 0.88\n",
      "-69.36598205566406 [0.01963549 0.5866324  0.39373213]\n",
      "Episode 700\tLast reward: 0.00\tAverage reward: 0.72\n",
      "32.91352081298828 [0.02198414 0.3055675  0.6724484 ]\n",
      "Episode 705\tLast reward: 0.95\tAverage reward: 0.77\n",
      "57.68339920043945 [0.04502835 0.10641587 0.8485558 ]\n",
      "Episode 710\tLast reward: 0.94\tAverage reward: 0.81\n",
      "10.102958679199219 [0.04227142 0.23182437 0.7259042 ]\n",
      "Episode 715\tLast reward: 0.96\tAverage reward: 0.76\n",
      "12.276154518127441 [0.04212241 0.06754614 0.8903315 ]\n",
      "Episode 720\tLast reward: 0.94\tAverage reward: 0.72\n",
      "90.7953872680664 [0.12329571 0.5082     0.3685043 ]\n",
      "Episode 725\tLast reward: 1.00\tAverage reward: 0.77\n",
      "88.9837646484375 [0.09633877 0.54073435 0.36292684]\n",
      "Episode 730\tLast reward: 0.95\tAverage reward: 0.77\n",
      "38.0883674621582 [0.05272356 0.05165625 0.89562017]\n",
      "Episode 735\tLast reward: 0.95\tAverage reward: 0.81\n",
      "18.758764266967773 [0.04700105 0.05166219 0.9013368 ]\n",
      "Episode 740\tLast reward: 0.96\tAverage reward: 0.80\n",
      "142.64004516601562 [0.04397884 0.06162884 0.89439225]\n",
      "Episode 745\tLast reward: 0.94\tAverage reward: 0.83\n",
      "87.53175354003906 [0.08410887 0.17198384 0.7439073 ]\n",
      "Episode 750\tLast reward: 0.94\tAverage reward: 0.85\n",
      "41.296512603759766 [0.09427538 0.5311624  0.3745622 ]\n",
      "Episode 755\tLast reward: 0.93\tAverage reward: 0.84\n",
      "20.98305320739746 [0.05113278 0.05195022 0.896917  ]\n",
      "Episode 760\tLast reward: 0.95\tAverage reward: 0.82\n",
      "90.88981628417969 [0.05866567 0.0760358  0.8652986 ]\n",
      "Episode 765\tLast reward: 0.94\tAverage reward: 0.85\n",
      "-71.09183502197266 [0.0574063  0.08324112 0.8593526 ]\n",
      "Episode 770\tLast reward: 0.93\tAverage reward: 0.74\n",
      "16.420246124267578 [0.0965904  0.5837483  0.31966135]\n",
      "Episode 775\tLast reward: 0.00\tAverage reward: 0.74\n",
      "44.41912841796875 [0.02707534 0.02818318 0.9447415 ]\n",
      "Episode 780\tLast reward: 0.97\tAverage reward: 0.79\n",
      "41.88709259033203 [0.06757912 0.42957103 0.5028498 ]\n",
      "Episode 785\tLast reward: 0.93\tAverage reward: 0.83\n",
      "19.586956024169922 [0.03484854 0.12553506 0.83961636]\n",
      "Episode 790\tLast reward: 0.94\tAverage reward: 0.82\n",
      "-4.109890937805176 [0.02549455 0.16708316 0.80742234]\n",
      "Episode 795\tLast reward: 0.00\tAverage reward: 0.80\n",
      "-23.75013542175293 [0.0414427  0.13057846 0.82797885]\n",
      "Episode 800\tLast reward: 0.98\tAverage reward: 0.79\n",
      "17.214752197265625 [0.01973526 0.02808862 0.95217615]\n",
      "Episode 805\tLast reward: 0.92\tAverage reward: 0.78\n",
      "30.992565155029297 [0.03380753 0.05020722 0.91598517]\n",
      "Episode 810\tLast reward: 0.94\tAverage reward: 0.82\n",
      "26.858652114868164 [0.04378687 0.5518412  0.4043719 ]\n",
      "Episode 815\tLast reward: 0.98\tAverage reward: 0.85\n",
      "28.754308700561523 [0.01004667 0.02029808 0.9696553 ]\n",
      "Episode 820\tLast reward: 0.95\tAverage reward: 0.88\n",
      "16.42355728149414 [0.01174673 0.07374559 0.9145077 ]\n",
      "Episode 825\tLast reward: 0.95\tAverage reward: 0.85\n",
      "49.94507598876953 [0.04831848 0.7063814  0.24530011]\n",
      "Episode 830\tLast reward: 0.93\tAverage reward: 0.87\n",
      "21.40911865234375 [0.0057528  0.01186867 0.98237854]\n",
      "Episode 835\tLast reward: 0.95\tAverage reward: 0.89\n",
      "21.766803741455078 [0.02133309 0.14859772 0.8300692 ]\n",
      "Episode 840\tLast reward: 0.89\tAverage reward: 0.90\n",
      "16.5091609954834 [0.0098068  0.08419667 0.9059965 ]\n",
      "Episode 845\tLast reward: 0.96\tAverage reward: 0.87\n",
      "3.2183609008789062 [0.02267065 0.7100277  0.26730165]\n",
      "Episode 850\tLast reward: 0.96\tAverage reward: 0.89\n",
      "-7.313592910766602 [0.02380935 0.78890675 0.18728386]\n",
      "Episode 855\tLast reward: 0.00\tAverage reward: 0.85\n",
      "-4.6101179122924805 [0.02377651 0.13321503 0.84300846]\n",
      "Episode 860\tLast reward: 0.98\tAverage reward: 0.84\n",
      "14.45191478729248 [0.01444245 0.07578848 0.909769  ]\n",
      "Episode 865\tLast reward: 0.96\tAverage reward: 0.86\n",
      "13.232654571533203 [0.014897   0.04593917 0.93916386]\n",
      "Episode 870\tLast reward: 0.95\tAverage reward: 0.89\n",
      "30.238067626953125 [0.05915754 0.34263995 0.5982026 ]\n",
      "Episode 875\tLast reward: 0.91\tAverage reward: 0.90\n",
      "27.197826385498047 [0.00675519 0.01025119 0.9829936 ]\n",
      "Episode 880\tLast reward: 0.93\tAverage reward: 0.91\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main() \n",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m):  \u001b[39m# Don't infinite loop while learning\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     action,show_prob \u001b[39m=\u001b[39m select_action(state)\n\u001b[1;32m----> 8\u001b[0m     state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m      9\u001b[0m     \u001b[39m#if args.render:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[39m#    env.render()\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     policy\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\gymnasium\\core.py:422\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    419\u001b[0m     \u001b[39mself\u001b[39m, action: ActType\n\u001b[0;32m    420\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    423\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:38\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\users\\fphub\\documents\\ibl\\miniworld\\miniworld\\envs\\oneroom.py:66\u001b[0m, in \u001b[0;36mOneRoom.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m---> 66\u001b[0m     obs, reward, termination, truncation, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox):\n\u001b[0;32m     69\u001b[0m         reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reward()\n",
      "File \u001b[1;32mc:\\users\\fphub\\documents\\ibl\\miniworld\\miniworld\\miniworld.py:717\u001b[0m, in \u001b[0;36mMiniWorldEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mcarrying\u001b[39m.\u001b[39mdir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mdir\n\u001b[0;32m    716\u001b[0m \u001b[39m# Generate the current camera image\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_obs()\n\u001b[0;32m    719\u001b[0m \u001b[39m# If the maximum time step count is reached\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_count \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_episode_steps:\n",
      "File \u001b[1;32mc:\\users\\fphub\\documents\\ibl\\miniworld\\miniworld\\miniworld.py:1200\u001b[0m, in \u001b[0;36mMiniWorldEnv.render_obs\u001b[1;34m(self, frame_buffer)\u001b[0m\n\u001b[0;32m   1188\u001b[0m glLoadIdentity()\n\u001b[0;32m   1189\u001b[0m gluLookAt(\n\u001b[0;32m   1190\u001b[0m     \u001b[39m# Eye position\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m     \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mcam_pos,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1197\u001b[0m     \u001b[39m0.0\u001b[39m,\n\u001b[0;32m   1198\u001b[0m )\n\u001b[1;32m-> 1200\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_world(frame_buffer, render_agent\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\users\\fphub\\documents\\ibl\\miniworld\\miniworld\\miniworld.py:1076\u001b[0m, in \u001b[0;36mMiniWorldEnv._render_world\u001b[1;34m(self, frame_buffer, render_agent)\u001b[0m\n\u001b[0;32m   1073\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mrender()\n\u001b[0;32m   1075\u001b[0m \u001b[39m# Resolve the rendered image into a numpy array\u001b[39;00m\n\u001b[1;32m-> 1076\u001b[0m img \u001b[39m=\u001b[39m frame_buffer\u001b[39m.\u001b[39;49mresolve()\n\u001b[0;32m   1078\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\users\\fphub\\documents\\ibl\\miniworld\\miniworld\\opengl.py:347\u001b[0m, in \u001b[0;36mFrameBuffer.resolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m glBindFramebuffer(GL_READ_FRAMEBUFFER, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_fbo)\n\u001b[0;32m    346\u001b[0m glBindFramebuffer(GL_DRAW_FRAMEBUFFER, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_fbo)\n\u001b[1;32m--> 347\u001b[0m glBlitFramebuffer(\n\u001b[0;32m    348\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    349\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    350\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwidth,\n\u001b[0;32m    351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheight,\n\u001b[0;32m    352\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    353\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwidth,\n\u001b[0;32m    355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheight,\n\u001b[0;32m    356\u001b[0m     GL_COLOR_BUFFER_BIT,\n\u001b[0;32m    357\u001b[0m     GL_LINEAR,\n\u001b[0;32m    358\u001b[0m )\n\u001b[0;32m    360\u001b[0m \u001b[39m# Resolve the depth component as well\u001b[39;00m\n\u001b[0;32m    361\u001b[0m glBlitFramebuffer(\n\u001b[0;32m    362\u001b[0m     \u001b[39m0\u001b[39m,\n\u001b[0;32m    363\u001b[0m     \u001b[39m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m     GL_NEAREST,\n\u001b[0;32m    372\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\pyglet\\gl\\lib.py:87\u001b[0m, in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mGLException\u001b[39;00m(\u001b[39mException\u001b[39;00m):\n\u001b[0;32m     84\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrcheck\u001b[39m(result, func, arguments):\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m _debug_gl_trace:\n\u001b[0;32m     89\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'miniworld_001.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "policy.load_state_dict(torch.load('miniworld_001.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0831, 0.6644, 0.2525]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0477, 0.8542, 0.0981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0311, 0.8346, 0.1343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.8363e-04, 4.9172e-04, 9.9922e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0012, 0.0018, 0.9970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0052, 0.0081, 0.9868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0357, 0.1140, 0.8503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0017, 0.0010, 0.9972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0032, 0.0025, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0042, 0.0150, 0.9808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0025, 0.0058, 0.9918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0012, 0.0030, 0.9958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0016, 0.0037, 0.9947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0012, 0.0025, 0.9963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[8.5892e-04, 1.2586e-03, 9.9788e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0022, 0.0040, 0.9939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0035, 0.0125, 0.9840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0064, 0.0434, 0.9502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0073, 0.0932, 0.8995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.8677e-05, 1.3284e-05, 9.9996e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.5277e-05, 5.3853e-06, 9.9998e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.2229e-05, 3.9133e-06, 9.9998e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.0619e-05, 3.4134e-06, 9.9999e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.5136e-05, 3.1430e-05, 9.9991e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.1598e-05, 1.7023e-05, 9.9995e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.8060e-05, 6.4825e-06, 9.9998e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.0129e-05, 7.6668e-06, 9.9997e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.7915e-05, 1.9451e-05, 9.9994e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.0499e-05, 1.8280e-05, 9.9995e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.7815e-05, 2.6336e-05, 9.9994e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.4509e-05, 1.5152e-05, 9.9996e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[4.8048e-05, 2.6620e-05, 9.9993e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.2560e-04, 6.1183e-05, 9.9981e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.0043e-03, 8.1212e-04, 9.9818e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0022, 0.0021, 0.9957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0049, 0.0050, 0.9901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[4.7575e-05, 1.6832e-05, 9.9994e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.6690e-05, 1.3677e-05, 9.9993e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.7816e-04, 4.2080e-05, 9.9978e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[4.2865e-04, 1.0588e-04, 9.9947e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0023, 0.0010, 0.9966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0047, 0.0025, 0.9928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0114, 0.0377, 0.9510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0115, 0.0410, 0.9475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0130, 0.0520, 0.9350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0102, 0.0375, 0.9523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0068, 0.0196, 0.9736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0050, 0.0159, 0.9792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0030, 0.0083, 0.9887]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0019, 0.0037, 0.9944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0025, 0.0051, 0.9924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0032, 0.0078, 0.9889]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0024, 0.0070, 0.9906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0019, 0.0032, 0.9949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0021, 0.0044, 0.9935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0017, 0.0041, 0.9941]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.0021, 0.9968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.0428e-04, 1.7175e-03, 9.9738e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.9858e-04, 1.4032e-03, 9.9790e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[4.7216e-04, 9.8441e-04, 9.9854e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0010, 0.0021, 0.9969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.0015, 0.9974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0010, 0.0019, 0.9971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[7.1026e-04, 1.2764e-03, 9.9801e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0015, 0.0041, 0.9944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0031, 0.0135, 0.9835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0031, 0.0125, 0.9844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0050, 0.0352, 0.9598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0048, 0.0419, 0.9533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0035, 0.0373, 0.9592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0132, 0.1982, 0.7886]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0201, 0.6816, 0.2983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[7.4215e-05, 4.6524e-05, 9.9988e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.0678e-04, 8.8927e-05, 9.9980e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5.3711e-05, 4.8522e-05, 9.9990e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.8655e-04, 2.0089e-04, 9.9961e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[7.0560e-05, 5.8594e-05, 9.9987e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.4636e-04, 2.7971e-04, 9.9937e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.7481e-04, 6.1723e-04, 9.9871e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0061, 0.0080, 0.9858]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0135, 0.0260, 0.9605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0310, 0.0633, 0.9057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[3.7915e-05, 2.3941e-05, 9.9994e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[4.0133e-05, 2.5585e-05, 9.9993e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.0246e-04, 6.2785e-05, 9.9983e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.2408e-04, 4.8624e-04, 9.9889e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0023, 0.0023, 0.9954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0043, 0.0066, 0.9891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0581, 0.2220, 0.7199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0573, 0.2277, 0.7150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0435, 0.2500, 0.7065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0411, 0.1754, 0.7835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0714, 0.5653, 0.3633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0707, 0.6773, 0.2520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0780, 0.6656, 0.2565]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0840, 0.6566, 0.2594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0830, 0.6609, 0.2560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0826, 0.6593, 0.2582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0842, 0.6551, 0.2607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0768, 0.6780, 0.2452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MiniWorld-OneRoom-v0', view=\"agent\", render_mode=\"human\")\n",
    "# env = GreyscaleWrapper(env)\n",
    "env = PyTorchObsWrapper(env)\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Create the display window\n",
    "env.render()\n",
    "\n",
    "for _ in range(100):\n",
    "    action,probs = select_action(observation) # agent policy that uses the observation and info\n",
    "    print(probs)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c269a8ac1ce32886f1d361a7d7752f19c27b7d8ebd8d0fb0e1cb07b13dddb563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
