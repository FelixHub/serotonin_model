{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import pyglet\n",
    "from pyglet.window import key\n",
    "\n",
    "import miniworld\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MiniWorld-OneRoom-v0', view=\"agent\", render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.hidden_size = 64\n",
    "        self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 8, 6, stride=2).to(device)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 6, stride=2).to(device)\n",
    "        self.maxpool = nn.MaxPool2d(4, stride=2).to(device)\n",
    "        \n",
    "        # self.conv1 = nn.Conv2d(3, 32, 4, stride=2).to(device)\n",
    "        # self.conv2 = nn.Conv2d(32, 64, 4, stride=2).to(device)\n",
    "        # self.conv3 = nn.Conv2d(64, 128, 4, stride=2).to(device)\n",
    "        # self.conv4 = nn.Conv2d(128, 256, 4, stride=2).to(device)\n",
    "\n",
    "        self.outconvsize = 16*5*7 # 3*256\n",
    "        \n",
    "        self.affine1 = nn.Linear( self.outconvsize , self.hidden_size).to(device) \n",
    "        \n",
    "        self.rnn = nn.RNN(self.hidden_size, self.hidden_size).to(device)\n",
    "        \n",
    "        self.affine2 = nn.Linear(self.hidden_size, 3).to(device)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = torch.zeros(1, 1, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(self.maxpool(x))\n",
    "        # x = F.relu(self.conv3(x))\n",
    "        # x = F.relu(self.conv4(x))\n",
    "        x = x.reshape(-1,self.outconvsize )\n",
    "        x = self.relu(self.affine1(x))\n",
    "        h = self.rnn(x.unsqueeze(0), self.hidden_state)[1]\n",
    "        self.hidden_state = h\n",
    "        action_scores = self.tanh(self.affine2(h))\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).permute(0,3,1,2).to(device)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item(),probs\n",
    "    \n",
    "gamma = 1\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + eps) # this create error ...\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    policy.reset_hidden_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50370"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in range(1000):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 100):  # Don't infinite loop while learning\n",
    "            action,probs = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            #if args.render:\n",
    "            #    env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 10 == 0:\n",
    "\n",
    "            \n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "            print(probs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 0.00\tAverage reward: 0.00\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Episode 10\tLast reward: 0.00\tAverage reward: 0.00\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Episode 20\tLast reward: 0.00\tAverage reward: 0.00\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Episode 30\tLast reward: 0.00\tAverage reward: 0.03\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Episode 40\tLast reward: 0.00\tAverage reward: 0.06\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Episode 50\tLast reward: 0.00\tAverage reward: 0.13\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Episode 60\tLast reward: 0.00\tAverage reward: 0.08\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Episode 70\tLast reward: 0.00\tAverage reward: 0.04\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main() \n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     15\u001b[0m running_reward \u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m \u001b[39m*\u001b[39m ep_reward \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39m0.05\u001b[39m) \u001b[39m*\u001b[39m running_reward\n\u001b[1;32m---> 16\u001b[0m finish_episode()\n\u001b[0;32m     17\u001b[0m \u001b[39mif\u001b[39;00m i_episode \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mLast reward: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mAverage reward: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     19\u001b[0m           i_episode, ep_reward, running_reward))\n",
      "Cell \u001b[1;32mIn[5], line 74\u001b[0m, in \u001b[0;36mfinish_episode\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     73\u001b[0m policy_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(policy_loss)\u001b[39m.\u001b[39msum()\n\u001b[1;32m---> 74\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     75\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     76\u001b[0m \u001b[39mdel\u001b[39;00m policy\u001b[39m.\u001b[39mrewards[:]\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fphub\\mambaforge\\envs\\deep_env\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "8\n",
      "8\n",
      "6\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "6\n",
      "7\n",
      "5\n",
      "0\n",
      "8\n",
      "8\n",
      "3\n",
      "2\n",
      "4\n",
      "7\n",
      "1\n",
      "4\n",
      "0\n",
      "2\n",
      "5\n",
      "2\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "9\n",
      "6\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "2\n",
      "0\n",
      "8\n",
      "2\n",
      "4\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "4\n",
      "6\n",
      "5\n",
      "2\n",
      "1\n",
      "4\n",
      "6\n",
      "0\n",
      "6\n",
      "9\n",
      "1\n",
      "0\n",
      "4\n",
      "3\n",
      "1\n",
      "7\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "5\n",
      "3\n",
      "8\n",
      "2\n",
      "9\n",
      "9\n",
      "5\n",
      "3\n",
      "8\n",
      "9\n",
      "4\n",
      "2\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "6\n",
      "9\n",
      "6\n",
      "3\n",
      "0\n",
      "4\n",
      "8\n",
      "7\n",
      "6\n",
      "1\n",
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MiniWorld-OneRoomS6-v0', view=\"agent\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Create the display window\n",
    "env.render()\n",
    "\n",
    "for _ in range(100):\n",
    "    action,_ = select_action(observation) # agent policy that uses the observation and info\n",
    "    print(action)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c269a8ac1ce32886f1d361a7d7752f19c27b7d8ebd8d0fb0e1cb07b13dddb563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
